\chapter{作为一个操作系统}

按伟大的计算原理一书的框架去整理：\hl{计算、记忆、通信、协作、设计和评估}。

CAP是分布式系统设计的指导原则。CA系统中C和A具有丰富的涵义，C可以是一致性、可靠性、安全性，A是可用性、性能、用户体验等。
设计要围绕着C/A进行权衡，评估要看C/A相关的指标。\hl{把性能纳入可用性的光谱之中，可靠性纳入一致性的光谱之中}。

平时看到的各类数据，如高可用架构、高可用MySQL、高性能架构等等，集群分类有\hl{HA集群、HP集群、LB集群}。
CAP把设计和评估统一了起来。\hl{稳定可靠高性能就是C/A的组合}。

Latency Numbers Every Programmers Show Know是思考的起点，包括了计算机各个子系统的性能参数。
多级存储层次是其中最复杂的部分：CPU高速缓存、内存、SSD、磁盘等。
NVM技术的发展，填补了内存和SSD之间的分界线。

网络的协议栈：RDMA、iSCSI/iSER、SCSI在协议栈中才能得到充分的理解，X over Y的结构。
NVMe over PCI-E，NVMe over Fabric等都可作此解。

\section{硬件}

硬件特性： 一台服务器有多个NUMA节点，每个NUMA节点包含多个core。
一个NUMA节点访问本地内存速度快、访问别的NUMA节点速度慢。
NUMA节点的访问特性对编程意味着什么？

每个NUMA节点对应有内存slot，但在OS层面看不到NUMA节点与内存之间的隶属关系。
虚拟地址可以连续，在用到内存的时候，映射到相应的物理page frame。OS应就近
分配物理内存页。

\begin{enumbox}
\item 列出所有CPU/core
\item 如何把线程绑定到core上(设置处理器亲和性)
\item core线程维护TLS和私有memory allocator
\item core与memory存在什么样的关系，最终表现为访存latency
\end{enumbox}

\section{Scheduler}

从操作系统的角度去理解协程。协程是用户态线程，不受kernel scheduler的调度，
\hl{协程也意味着执行的不连续性, 1+N}，与context switch。
协程有独立的调度器，负责从就绪队列里选出下一个要执行的任务。

站在scheduler的角度看，coroutine并无大的不同，只是实现了yield和resume方法。
通过任务队列进行调度，选出下一个要运行的协程。
协程是非抢占的，没有yield的话，在执行完后返回。

站在线程执行的角度看，scheduler和其它任务也无不同，看指令流顺序执行。
这是回到执行过程本质的观点。

\subsection{实现}

系统可以指定几个core，专门处理IO。每个core关联一个core thread，运行scheduler代码。

本质上，所谓的scheduler，就运行特定指令序列的操作系统线程，通过操作特定的数据结构和上下文切换，
模拟协程行为。

别的线程要与core thread通信，需要进行同步。core thread内部，不需要同步措施。

\hl{调度器和协程代码，交错执行}。调度器切换到task，则执行task指令；
task让出控制权，或者返回，则执行调度器指令。

创建task，通过makecontext管理task要执行的过程，通过swapcontext在调度器和task之间切换上下文。

task的生灭过程，一个task的生命周期，状态变迁：FREE, RUNABLE/READY, RUNNING, SUSPEND, BACKTRACE。

BACKTRACE用于打印日志，扫描每一个task，检查task的存活时间。目前，不允许有特长时间的任务存在。
BACKTRACE是由调度器发起的。BACKTRACE不检查sleep状态的task。

每个task，有parent，从而构成task树。

目前维护有三个task队列(runable, reply\_local, reply\_remote)，两个新的申请队列(wait\_task, request\_queue)。

task队列占用tasks槽位，总数受限，\hl{目前默认为1024个槽位}。

调度器支持优先级队列：引入不同于runable的队列，多队列之间分配时间片。

Actor编程模型，SEDA高并发架构。

\subsection{FAQ}

\begin{compactenum}
\item schedule\_yield1 timeout。检查的是\hl{从yield到再次唤醒的时间，而不是任务的age，一个task可以多次yield/resume}
\item tasks槽位占满后，再加入的请求会放入wait list。task和wait list之间，可能形成相互依赖的deadlock。
\item schedule\_task\_get和schedule\_yield1必须配对使用
\end{compactenum}

\section{内存}

要解决的问题，一并发下的扩展性；二是碎片化。

两层内存allocator：通过buddy allocator解决外部碎片，通过slab allocator解决内部碎片。
在page allocator的基础上采用object pool模式。

每个core线程拥有私有内存，但不应静态分配一个固定值，而是动态按需分配。
\hl{buddy算法如何管理动态变化的内存}？
因为每个core线程所需内存量可能不均衡，出现总量够，而单个core内存不足的情况。

malloc返回的是虚拟地址，还没有分配物理地址。

\begin{enumbox}
\item page table
\item TLB
\item cache line
\item false share
\end{enumbox}

预留、总量

优化项：
\begin{enumbox}
\item O3编译选项
\item 去掉debug和info信息
\item inline
\item IO路径上的函数放入同一代码段
% \item 减少代码体积
\item variable\_t
\item lock table
\item hash table
\item replica\_srv\_get
\end{enumbox}

分析工具：
\begin{enumbox}
\item perf
\item lmbench
\end{enumbox}

\subsection{Page Allocator}

一次从OS申请固定数量的hugepage，用buddy算法管理其alloc、free过程。
所谓buddy，就是管理一段连续内存单元，随着alloc、free的过程进行拆分与合并的规则。
最开始的时候，整个区域是连续的、每个节点跟踪记录所能分配的最大连续内存单元。

\subsection{Mem\_Cache}

ring\_pool是固定大小的内存池，可以循环使用。

两种分类
\begin{enumbox}
\item 分为两类：core线程私有与公共部分。
\item slab，分类，每类固定大小，可以自如伸缩。
\end{enumbox}

\subsection{Others}

\begin{enumbox}
\item jemalloc
\item tcmalloc
\item boost::pool
\end{enumbox}

\section{控制器}

目录和卷，都通过controller进行管理。目录controller的概念有待进一步完善。

每个控制器都可以看着一chunk树，其根节点对应chunk的副本位置列表，决定了控制器的位置：列表中第一副本所在节点。
在迁移控制器时，同样需要遵循该规则，所以需要先调整根chunk的chkinfo。

所有卷的操作都需要通过controller进行。客户端在访问一个卷时，第一步要找到该卷控制器的所在节点nid，
然后把nid作为参数传入后续调用中，如nid是客户端，则进程内；否则，发起rpc调用。
本地访问也可走rpc，可以利用rpc timeout等特性。

md\_map是控制器位置缓存，如可以在cache里找到，直接返回。如缓存不命中，则发起UDP广播。
每个lichd进程有独立线程监听端口：20915。检查cluster uuid，magic，crc等匹配后，尝试加载控制器，然后做出回应。
发起UDP广播的客户端收集各lichd进程的响应，如找到匹配的nid，可以直接退出该过程。

控制器加载过程：第一副本非本节点，返回EREMCHG。加载成功后，用vctl缓存管理起来，缓存项带引用计数和删除标志。
目前，采用lease机制保证vctl的唯一性。其必要性可进一步推演。
加载过程需要保证并发下的唯一性，如果有多个task发起加载过程，只有一个实际执行，别的进入等待队列。
加载完成后，唤醒等待队列里的任务。

\section{I/O}

\subsection{Disk Allocator}

\subsection{Sqlite}

\subsection{FS}

\subsection{SPDK}

因为提交性能极高，不会block主线程，不需采用AIO。

\section{通信}

SCSI是节点与设备，节点与节点之间的通信协议。
SCSI over TCP/IP就是iSCSI，即SCSI/iSCSI/TCP协议栈，iSCSI over RDMA是iSER。

NVMe与SCSI是竞争协议，是演化方向。节点与设备：NVMe over PCIe。
节点之间则是NVMe over fabric（NVMf）。fabric与PCIe是不同的连接通道，泛指节点之间的通信链路。

\subsection{RPC}

封装底层不同链路，如TCP、或RDMA，提供统一的通信入口。

VM与target，可采用vhost方式，即IPC。

\subsection{RDMA}

一是initiator与target之间(SCSI或NVMe协议)；二是各节点的corenet之间(什么协议？)。

RNIC offload相关功能，跳过kernel直接访问应用地址空间。
需要应用向RNIC注册自己。此数据传输过程不需要CPU参与。
