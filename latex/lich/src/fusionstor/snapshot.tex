\chapter{Snapshot}

抽象出AbstractVolume，Volume、Snapshot、Clone都是其子类。

\hl{卷或克隆卷扩容后，快照没有相应扩容，会返回ENOENT，需要填充0}。

\section{问题}

\begin{enumbox}
\item session方式，无commit
\item 随机IO，小cache，频繁swap，有问题
\item 如果标记dirty，会出现all dirty的情况
\item 如果有COW，需要pin住源
\end{enumbox}

\section{操作}

需要支持的快照操作：
\begin{enumbox}
\item create
\item rm
\item list
\item rollback
\item clone(跨卷read)
\item read
\item flatten(存储池迁移和复制，需要实现该接口)
\item protect/unprotect
\end{enumbox}

\begin{tabular}{|s|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}| }
    \hline
    \rowcolor{lightgray} \multicolumn{5}{|c|} {Snapshot} \\
    \hline
    Feature & COW & ROW1 & ROW2 & LSV \\
    \hline
    snap tree & N &  & & \\
    \hline
    snap create & N &  & & \\
    \hline
    snap rm -> gc & N &  & & \\
    \hline
    snap rollback & N &  & & \\
    \hline
    snap clone -> read & N & & & \\
    \hline
    write & N &  & & \\
    \hline
    read & & N & & \\
    \hline
    space & N &  & & \\
    \hline
    consistency group & N &  & & \\
    \hline
\end{tabular}

采用COW，ROW或两者的组合形式，各有优缺点。

COW方式的快照，卷有完整的索引结构。别的快照点，只有增量的索引结构和发生更新的数据。
每个快照，存储的是创建之后，到下一个快照点之间发生更新的所有数据块。所以需要尽量降低发生copy的开销。
适用于频繁且具有局部性的热点负载场景，固定时间段内，每次copy的开销以及\textcolor{red}{复制集的大小}。

ROW方式下，如果meta不发生COW，新的快照点并无完整的索引结构，读过程需要沿着快照链向上回溯。
并且，rm，rollback等操作需要合并快照点。

如果发生了COW，索引项和数据项引用关系不再是1:1，而是多对多，需要专门的GC机制。
但rm，rollback等操作实现起来变得简单。

数据项的粒度，定长或变长，不同的负载，读写性能有不同的影响。
发生COW的粒度，索引结构管理的粒度决定了数据项的粒度。

IO粒度和数据项粒度的关系，如果IO粒度大于数据项粒度

如果IO粒度小于数据项粒度

这条规则是否永远成立：\textcolor{red}{快照树的任一路径，都需要具有完整的索引结构和数据项集，可以有冗余和共享}。

\section{COW}

为了支持快照树，需要分析rollback和读快照的过程。

\subsection{Create}

卷，clone卷，快照具有相同的索引结构，不同在于fileinfo的attr。

卷和快照的所有chunk，都是私有的，无法通过索引结构有效地定位chunk的位置。

创建快照是相当简单的过程，创建索引结构，设置正确的fileinfo，同时复制xattr。
卷的snap\_from指向其挂载的快照。

在写入卷的时候，发生COW过程，被修改的chunk先复制到卷的最近一个快照的私有存储空间里，
然后在卷上进行in place写。

\subsection{Rollback}

rollback，卷为源，以目标快照的内容覆盖源卷的内容。同时，为了不丢失信息，需要提前保存一定的数据。
需要保存什么数据？沿着快照树的根，到卷的当前快照(所在分支的最后一个快照)，\hl{所有修改过的chunk}。

卷的当前快照：只有在卷的当前快照是叶子节点的时候，才需要保持数据。

读快照的规则：
\begin{compactenum}
\item 自身
\item 下游
\item 回溯至卷(先回溯至公共父节点，然后到卷)
\end{compactenum}

前两步是为了读取修改过的chunk，后一步是为了读取所在分支上从来没有修改过的chunk。
两者合起来，代表快照的数据。

通过fileinfo的snap\_from建立快照树，通过snap\_version为每个快照分配id，通过snap\_rollback指定要回滚的目标快照版本号。

rollback的状态变迁

rollback过程依赖于两个快照路径：
\begin{compactenum}
\item root snap list
\item cross snap list
\end{compactenum}


\section{ROW}

% ROW与LSV的不同，在于同一LBA，只对应一个数据项，而不是对应多个版本的数据项。
% 但这个数据项的引用计数，可以为0，1，或多个。

ROW，快照会产生一个数据项的多个版本。LSV，除了快照之外，一个LBA的覆盖写入也会产生多个版本。

卷和快照，共存于底层lich卷，是否有问题？

元数据的\hl{基本管理单元是page}。按page来组织，每个page unit：chunkid+pageid。
在ROW的过程中，可以只改变一页到新chunk。而bitmap则发生了COW，copy 1M，改变其中一项。
(用chunkid+ page bitmap的方式行不通)。\hl{有一种特殊情况，bitmap管理了连续1M的数据，此时chunkid是重复的}。
如无页级元数据，则需要发生1M的COW。有页级元数据，引入了优化的机会。

bitmap管理1M对应底层1M。管理所需元数据：chunkid+各页的元数据。

snap tree, snap, chunk, page构成四级结构。LSV时，没有chunk那一级（写入位置模式）。

写入模式：顺序写入，需要维护当前写入点。1M模式，需要1M：1M的映射, 而不再需要chunk内页索引。
引用关系是页级的，写入模式是chunk级的。降低了ROW的写入粒度。

vvol：clone时用，跨卷引用。支持多级clone。clone与snapshot基本一致，不同在于跨卷读取。

写入的过程：会检查每个isref，入为1，则表明该chunk归属于本快照，否则，需要创建新的。
对L2 bitmap言，发生1M的COW；对LOG言，申请分配1M，同时写入。

cache一致性问题：

session聚合写入：

CC： 页锁，bitmap单元锁，COW锁。\hl{细粒度的锁，无法保护更大粒度锁上的并发操作}，如COW过程，ROW过程，
IO锁最好是区间锁。bitmap单元的提交操作。

页式管理的局限性：连续的有信息冗余。

空间管理：\hl{逻辑空间和物理空间的关系，物理空间的扩展性}。

\subsection{IO过程}

\begin{itembox}
\item CC: 范围锁，层次锁
\item 需要的实际IO次数
\item 页对齐
\item 跨chunk边界
\item 重入分析(数据更新，bitmap更新)
\end{itembox}

两个并发IO，如果落入一个1M，会访问到同一bitmap单元。
应先锁定bitmap单元，再锁定IO范围。遵循树状锁协议。
参考\hl{数据库索引结构上的锁协议}。

\begin{tcolorbox}

假设有新旧两个快照点S1, S2，每个快照点包括meta和data，提炼出的几个引导性问题：

\begin{enumerate}
    \item 数据块的粒度，page，chunk，extent？
    \item 每个快照点是否有完整的索引结构？
    \item 卷（写入点）是否有完整的索引结构？
    \item 哪一个是写入点？
    \item meta是否发生COW？
    \item data是否发生COW？
    \item 快照操作(create, rm, rollback, clone, flatten, ls, cat, protect/unprotect)的复杂度？
\end{enumerate}

\end{tcolorbox}

\section{ROW2}

\subsection{快照树}

快照树的每个节点，都是一个完整的索引结构。其中一个代表着当前写入点，写入点代表着卷。
回滚操作会改变当前写入点。

统一一下，卷和快照具有相同的索引结构。

每个快照结构有唯一的ID，是随着创建快照的过程递增的，卷快照/写入点具有最大的snap id。
LSV log结构记录了每页的snap id。

一颗树的节点，可以分为三类：
\begin{compactenum}
\item 根节点
\item 中间节点（有无分支）
\item 叶子节点
\end{compactenum}

\subsection{创建快照}

原有写入点变成只读，创建新的写入点，并复制L1元数据。

\subsection{删除快照}

快照树上不同的节点，需要不同的删除过程。对于叶子节点，直接删除即可，\textcolor{red}{需要回收数据吗}？

回收快照，涉及meta和data两个部分。

不应改变父快照的内容
保留子节点的共享内容

分两个节点：\hl{标记和回收}。

\subsection{列出快照树}

\subsection{写}

写是ROW的重定向过程，可能发生复制第二层元数据的过程。

如果连续写入，需记录当前的写入点。chunk内数据是逻辑不连续的，或者说，逻辑上连续的在chunk内是随机化了。

如果非连续写入，每页的写入位置计算得到，chunk内数据是逻辑连续的。大范围随机写入的情况下，需要分配很多chunk。
\textcolor{red}{需大力优化chunk的分配过程}。

pagetable是逻辑连续的，按逻辑空间组织。如果已分配位置，则覆盖写入。如果没有分配位置，如何分配？

逻辑地址怎么映射到物理地址？页式，段页式

\hl{写入位置，有三种分配策略：ROW2，ROW3，LSV}，都是按页粒度管理物理卷空间。

\subsection{回滚}

回滚并不会重用回滚到的快照点，而是相当于把写入点嫁接到目标快照点。写入点本身是一个独立的快照结构。

除了写入点的所有快照点，都处于只读状态，没有任何操作可以改变其状态。

回滚后的写入过程

\subsection{读}

如果不复制元数据，ROW实现的读过程需要回溯快照树，性能不佳。
如果复制元数据，则每一快照点都具有完整的索引结构，可以做到一次即可定位。

复制元数据，快照和数据具有多对多的引用关系，相当于共享数据块。

读优化： 元数据可以一次定位，但可能碎片化，沿着快照链往上读取。可以通过flatten的过程优化。

\subsection{Clone}

卷和其上的快照位于同一个物理卷内，所以不涉及跨卷读。clone后，会用到跨卷读快照，
类似于ROW过程，所以，创建和clone过程具有相似性。

cat, protect, unprotect, flatten

\subsection{GC}

GC过程和引用计数。每个记录的is\_ref记录是否写入了数据。可以回收写入点，但无法回收一般快照。

LSV: 逐个扫描每个log。

\section{实现}

快照列表信息记录在卷table\_proto的smap和sinfo区，info区的每一项包括name和chkinfo，还包括snap\_attr\_t。
smap用bit标记该位置是否有效。卷上的所有快照，构成快照树。

每个快照有自己的snap\_version和snap\_from，分别表示自己的版本号和父节点的版本号。

新创建的快照，采用卷当时的版本号和父版本号，创建成功后，更新卷版本号和父版本号，rollback版本号与卷版本号一致。

revert的时候，rollback版本号表示目标快照的版本号，如有autosnap生成，采用卷当时的版本号和父版本号。
完成后，更新卷的版本号，父版本号和rollback版本号。

卷的每一chunk具有自身的版本号，revert成功后，所有chunk的版本都等于目标快照版本号。

\subsection{rollback}

主要通过4个变量来控制rollback过程：fileinfo(snap\_version)。

/pool1/system/rollback/vol.32.0

\section{Bitmap Cache}

要解决的关键问题有：
\begin{enumbox}
\item cache一致性
\item 并发与事务
\end{enumbox}

问题导向
\begin{enumbox}
\item 这是最好的设计/实现吗？
\item 需要维护什么序？哪些操作可以并发？
\item chunk的is\_ref与bunit的ref什么关系？
\item 怎么区分snapshot与clone两种情况？
\end{enumbox}

chunk的is\_ref表示引用别的快照的L2 bitmap chunk。
在创建snapshot时，设为1。发生COW后，设为0。

如果chunk的is\_ref == 1，则读出的bunit.ref也等于1。
(发生COW后)
如果chunk的is\_ref == 0，则读出的bunit.ref可能有两种值。

在COW后，chunk的is\_ref等于0，对应的所有bunit都等于1。随着卷上发生写操作，对应的bunit也变为0。
所以两者的含义都是\hl{所指向的对象是否属于卷本身}，用于判断COW与数据写入过程是否需要分配数据块的条件，
两者都需要持久化。

每个快照对应一个bitmap，用于映射每个数据页的LBA到底层物理地址。
bitmap分两层：第一层私有，第二层在各个快照之间共享。
在写的时候，触发COW过程，从父快照copy对应的chunk，然后才能写入。

一个卷的多个快照，只有一个快照是可写的，其它快照都处于Read-only状态。
所有写的位置，必须是卷本身的数据空间。所以，对任一数据chunk，都是在可写快照时分配的，创建快照后冻结，产生新的写入点。

为了提高bitmap的存取性能，引入bitmap cache，按页组织(可以类比于VFS的page cache，每个inode对应一个radix tree)。
更新bitmap的粒度是bunit，通常为8B，对应一个底层物理位置，一次io可能对应多个bunit，甚至可能跨越page边界。

\subsection{Read}

\subsection{Write}

IO写的时候，对应的每一个bunit，都存在四种状态：
\begin{enumbox}
\item 本地
\item 没有分配
\item 指向某快照
\item 克隆卷，指向克隆卷的源快照
\end{enumbox}

后三种情况统一处理。

如果是单页更新过程，

如果是多页更新过程，

\section{ConsistencyGroup}

使用场景：如一个分布式数据库，log写入一个卷，数据写入另一个卷，如何打快照才能不违反应用层的一致性？

LVM的快照，涉及底层多个物理卷。clvm

ceph组内各卷，共用一个journal，性能低、架构不灵活

CG是一个独立实体，创建后为空，可以把一些卷加进来，按组进行快照操作。
