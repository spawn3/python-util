\chapter{2019}

\section{01}

\subsection{08}

用本文档记录日志，只维护一个即可。growth记录更高层次的指导原则，一般来说相对比较稳定。
如此就形成了一个相互促进的双环架构。

不要自我设限，一定如何如何？打基础，看机遇。

一些指导原则
\begin{enumbox}
\item 概念图、重要的是理解核心概念，费曼学习法，用自己的语言描述出来，取象比类，借助类比、联想等思维方式。
\item 在默认模式/执行模式，走神/专注等进行切换，\hl{文武之道、一张一弛}，有助于激发出创造力。
\item 强调综合判断能力。
\item 慢练
\end{enumbox}

体系架构、操作系统和汇编语言是底层逻辑。数据结构、算法、编程语言结合起来进行学习。

坚持专业技术方面的发展，\hl{CBA都要有所涉猎}。现在正是最佳时机，不能再没有重点了。

重点应放在做过的项目和全闪上。\hl{多路径？多存储网段？SPDK、NVMe}等。
比如单机存储引擎，用sqlite3、redis、RocksDB，为什么？

从势、道、法、术、器等维度进行梳理。

\subsection{09}

clone后，没把源快照置为protected状态。

\hl{software pipelines和排队论是性能分析利器}。SEDA、actor和lambda是架构方法。
图论、网络流，老三论。按中医理论，人体由藏象经络组成，运行气血。与网络系统有很强的相似性。
图是最复杂的数据结构，降维去看其它数据结构会如何？petri net和有限状态机。
把以前生活中经历的点连成线，会更明晰。

专业的力量、能力变现、价值规律、写作是最好的自我投资、自我进化、放大核心优势都是很好的理念，关键在落实。

本质上是核模式的应用。先在一个小的专业领域建立根据地，再顺势裂变扩张。

眼观六路耳听八方，行业格局分析至关重要，技术加商业两条腿走路。

自己要变得强大，这是积极心态，接下来是如何才能变得强大，这是现实，
要做现实的造梦者，不做桃花源中人。

lich的io 路径需要控制器中转，是否影响性能？client直接读写数据会如何？ceph client与osd primary直接通信。

只有一个RDMA网卡，存在硬件上限。如何使用多个RDMA网卡呢？8k 100w iops。

接触斯多葛主义。内心的自由和平静。

Oracle asm?

\subsection{10}

向身边的人学习，虚心，不要老端着，自认为了不起，其实需要理解的东西很多。
差不多先生，含混不清是极大的弊端和缺陷，一定要清晰，清晰才有力量。
表达要简洁清澈清晰。空性不代表含混，而是包容。

写作是最好的自我投资，要深刻理解这句话，并每日打磨写作技能。
先记下流水账，固定时间进行归纳整理，提炼升华。

做事情太保守，\hl{视野不够高，格局不够大}，现在呢？更多自以为是，
需要沉下心来，好好观察、琢磨。看清趋势。正心诚意，取势、明道、优术，然后是利器、举例。

集中一段时间，专攻技术。围绕关键问题，从广度和深度，\hl{致广大而尽精微}。
技术综述好做，选几本书多读几遍即可，难的还是深度。另外一条途径，就是多与人聊技术。

围绕全闪进行写作。全力以赴吧。

\hrulefill

\hl{重点整理在HY做过的工作}：
\begin{enumbox}
\item snapshot and LSV
\item Recovery
\item Balance
\item QoS
\item ***
\item ETCD
\item Network(TCP and RDMA)
\item ***
\item Scheduler
\item Memory Allocator
\item Disk Management (Local Storage Engine, +NVMe)
\item ***
\item iSCSI/iSER/NVMf
\end{enumbox}

\hl{恢复、平衡、删卷/snapshot、rollback、clone、flat}等操作，如何用统一的任务管理系统进行管理。
可以借鉴k8s等集群管理系统的经验。有了总控，可以更好地加入策略。

\hl{恢复和平衡}有些逻辑是通用的，如按pool处理，检测本地vctl变化，扫描本地vctl。
可以放到一个处理框架内进行处理。ceph如何进行数据平衡呢？

太大的调度粒度(vctl)不利于任务调度和负载均衡。qos如同节流阀。可以放到不同的位置上。

\hl{if-what}当前的恢复处理逻辑，如果发生\hl{网络分区故障}，会如何？

为了支持多网卡，需要做什么？

为了支持多路径，需要做什么？

\hrulefill

研读google发表的系统相关论文，看真实需求以及技术演进方向。
性能、稳定可靠、容错高可用、可扩展是几大非功能属性，决定产品品质。

理解google的技术体系，不能老是泛泛而谈。borg衍生出k8s。
\hl{hdfs、bigtable、megastore、mapreduce}都已经实现了容器化和统一管理。

有bigtable的sstable演化出leveldb和rocksdb。redis可以与rocksdb进行组合，改变全内存数据库的局限性。

催生了hadoop生态，进一步衍生了spark等lambda架构的统一计算平台。

\hrulefill

光点图灵

RAID之上有LVM，都是单机环境下的产物。虚拟化程序越来越高。进而是盘阵，然后是分布式架构,
scale-out越来越强，灵活性越来越高，软件定义和超融合由此而来。

MongoDB直接做RAID0，然后利用其自身的集群能力，master/slave replication架构，
可以failover and failback。这相当于内置存储。当时对存储层面的理解相当有限。

对gridfs扩容是如何做的？通过多个目录做的，也不设计磁盘虚拟化方面，如LVM。
面对的就是一个扩展性问题。

\hl{迁入阿里云、采用七牛对象存储后}，不自己维护服务器，避免了很大的麻烦。

\hrulefill

\hl{美地森的工作和成长经历}：

很长时间都理解不离aio、事件驱动的实质所在。
做技术确实死磕精神，重视贪多求大，结果反而欲速不达。

当时做edog又是如何做的呢？有什么可以反思的？
集群管理采用了Erlang，通过libvirt对接qemu，改了qemu的驱动，接入yfs作为底层存储。

对网络理解不够透彻，接触也少。前端时间获取交换机管理信息，技术收到都是类似的。
不过关于交换机，很多技术细节理解不到位。\hl{天马行空惯了，细节把握不到位}。
需要改进，做技术要认真再认真，程序化，每日记录、反思、总结、提炼。
既要有广度，深度更是必不可少。\hl{专精一件事，就可以立于不败之地}。
找到了自己的哲学、守中致和，核模式，就要学以致用，一以贯之，用来指导自己的言行。

接触了hadoop，为什么没有坚持呢？放着这么好的东西，不去跟进，
只是习惯于做些乱七八糟的东西，实在是有眼无珠呀。

虽然也有跟进，\hl{不坚定，不深入}，当然更谈不上真的掌握。

\hrulefill

华胜天成：法国电信做手机的SIP协议，管理软件，一头雾水。

包括以前在\hl{天地伟业、鸿业科技}，都没有深入业务，总是局限在一个小小的技术视角。
现在软硬件的变化，真是日新月异，不进则退。\hl{没有一个好的平台和机遇}，就很难崛起。
如何把握好的平台和机遇，当然要靠本日一点一滴的积累，磨炼基本功，把握大趋势。

鸿业科技做的\hl{官网计算}回忆起来，倒是给了很好的启示，说明了管道理论的普遍性。

一是个人成长放在首位，接着就是睁大眼睛，静静地坚定地去找团队、平台和机遇。（\hl{人和事的胜任度}）
两者匹配了，能力胜任度高，自然一切水到渠成。所以也不必着急焦虑，反反复复磨炼自己就够了，
金子总会发光的，好酒也怕巷子深。这些看似对立的格言，其实包含了深刻智慧。

这几年，技术发展真是太快了。风口一个也没把握住，原因何在呢？

\hl{接下来干什么呢}？

转移关注点，放低姿态，学习关键知识点，并记录在案。从整体系统中抽离，聚焦于更小的模块。

最近老王他们进行的性能优化工作是值得关注的架构调整。支持多路径，多网卡，甚至插到不同的交换机port上对性能都有影响(8x or 16x)，以提升性能。
性能的理论上限如何预估？

更多地转变到SPDK(NVMe, NVMf)、RDMA上面来。全NVMe方案如何？怎么构建全局共享，client与数据不同于控制器直接存取？
可以参考的资源有什么？

纪翠叶问及fio的部分参数什么含义？这是器层面的知识点。

\subsection{11}

中庸是我的圣经。从心道物等诸多维度确定了必要的理念。

\hrulefill

不深入理解传统存储的高度和局限，就不能理解很多核心概念，比如多路径、共享、全局缓存等。
分布式架构在\hl{扩展性、灵活性}方面胜出。

通过真假latency，可以估计等效并发。方法如下，单并发测量固定时延。多并发测量iops，取倒数即为假时延。
固定时延和假时延的比值就是该配置下的等效并发度。

io path也有串行和并行两部分，从而引入加速比的概念，上面的等效并发度就是加速比。
可以评估整个体系结构的并行执行效果。

按pipelines理解体系结构，体系结构也是网中网的分形结构。
官网、电路等等可以作为分析模型，从流体动力学和物理学里吸收能量。
但也不必把问题复杂化。从最少的元素推导出更多的规则，解释更多的现象，是科学的研究方法。

从体系结构、操作系统的原理和知识，去深入存储子系统的相关理解。

\hrulefill

\hl{网络分区}下，恢复、平衡和一般的IO是如何进行的？

比如一个5节点集群，采用2副本策略。分区意味着什么呢？怎么理解多数派这一要求。
5节点2副本，也是只能容忍单个节点故障，\hl{故障容错度是有副本数决定的}，与集群规模无关。
比如一个10节点的集群，2副本的情况下也只能容忍一个节点故障？

可以通过划分故障域改进这一点。副本数据分布到不同的故障域，容错按故障域来定义。

保护域和pool是等价的概念，保护域是一种物理隔离机制。把一个大集群分拆成小集群。所以基本的嵌套关系是\hl{集群-保护域-故障域-节点-磁盘}。

网络的VLAN是否也是一样的分区机制？

这一点与zk、etcd等协同共识系统存在很大的不同。

MySQL master/slave架构，能否自动切换？

\hrulefill

client直接与数据块通信，lich需要通过vctl中转，如何克服这一点？

多网卡支持，通过多网卡连接控制器，MP软件用来管理导出的LUN，确保只有一个。

SSD FTL中心任务是映射管理，与OS的页表、LSV的快照管理是一样的，关键是维护一个虚拟地址到物理地址的映射。
可以是一级，也可以是多级。\hl{伟大的计算原理}里面，讨论存储的时候提到四点：\hl{命名，映射，定位，授权}。
可以按此四个维度去思考林林总总的存储系统。

\hl{按主体-对象模型，授权有两种}：CL和ACL。CL以主体为中心，ACL以对象为中心（如文件系统的ACL）。

\hl{全力投入AFA}，大量阅读，深度思考。知识就是力量，这点认识还是不够深刻。

地址空间管理，抽象成了一切皆文件。内存、磁盘、文件都是地址空间。文件系统的设计需要充分地认识到这一点。
SSD最核心的功能也是这一点。SSD FTL很类似于LSV，很多可类比的地方，如映射、GC。

数据结构主要也是映射，如hash、map、graph，graph是最通用的映射关系。
再如，函数也是映射。lambda架构也提及此，map-reduce更成了并行计算模式之一。

\hrulefill

多快好省，快和好是当前重点，多和省在此基础上，迭代优化。\hl{性能、负载、高可用}是几种集群形式，
bonding，多网卡支持。bonding具有什么特征？只是HA？而不能同时工作，聚合性能和均分负载？

\hl{全力投入AFA}，大量阅读，深度思考。知识就是力量，这点认识还是不够深刻。

\subsection{12}

地址空间，如内存，文件都采用地址空间概念。多级页表可以支持稀疏地址空间，页表与inode采用的radix tree什么区别？
lich卷的chunk tree也采用了同样的结构。这样，需要维护虚拟地址到物理地址的映射。

对计算机体系结构、指令集、操作系统、汇编、编程语言有了初步的理解。对存储、网络、数据库也有一定的理解。
接下来就是围绕算法和数据结构为中心，\hl{以问题为主线}, 穿起来进行思考。伟大的计算原理给出了很好的总结。

分层去理解更透彻，如计算机组成结构化方法里，分为七层，网络也是分为若干层。

大的问题主要有调度、内存管理、地址空间管理、通信和同步、事务、分布式算法等。

以数据结构和算法为核心，去解决各类系统的核心问题。问题具有普遍性，算法也应理解为通用的算法。问题和算法是对应的。

提出问题，然后看应该如何去解题。解决问题是价值输出的重要方式。

分析io path就可以把一切知识点贯通了。一个写、一个读。集中在数据平面的相关问题。

\subsection{13}

在华云和光点的工作具有互补性，麻雀虽小五脏俱全，需要得到更好的总结、提炼和升华，一花一世界，由此去理解更广阔的技术世界。
譬如一个同心圆，虽有规模效应，规模引起一些质变，但稳固的核心，扎实的基本功，确是最为重要的，如此就拥有了强大的迁移能力。

不能不重视写作。

\hl{单一要素最大化，其它要素最小化}。算法可以作为下一阶段的单一要素，是不变的一。
算法的学习和理解脱离不了具体系统，一以贯之，就是以算法贯穿技术学习活动。

这里的算法是广义上的算法，不仅包括教科书里常常提到的算法，也包括协议、调度、事务等重要概念相关的算法。

\subsection{14}

\hl{化繁为简，打破学科边界}，从实际问题及其方案开始理解。深入理解计算机系统即是采用这种叙述法。

计算机系统可分为cpu(\hl{进程、并发、中断、隔离})、memory、io子系统(storage/fs and network)等，按此结构循序展开。
io又分存储和网络等，组成tree架构。关键组件：\hl{调度器(thread、io)，空间分配器（cache、memory、disk）}。

采用流一元论(flow dynamics)的描述框架，cpu居中调度，上下文切换和内存copy，可称为指令分配器。
\hl{flow由节点和边组成}，节点是处理单元或节流阀，边是管道。处理方式可以有多种：转发、映射、压缩、消重等。

衡量流效率的指标是latency、iops、throughput等。流的平衡态由平衡方程描述，输入过多，会导致处理不过来，
发生拥塞、溢出等故障，故需要限流/节流。

memory copy消耗总线带宽，bus是如何利用的？

先理解一般case，加入cc和故障恢复，系统就变得复杂了。虚拟化、cc、持久性是操作系统研究的三大块。

cpu亲和性需要通盘考虑，不仅仅是内存，还有disk、network，读设计线程和buffer，是否与core线程处于一个NUMA节点？
还是跨NUMA节点进行内存copy？ aio线程尽量与core线程距离近。\hl{如果只要一块网卡，则网卡属于哪个NUMA节点呢}？

\hrulefill

\hl{数据结构的内存layout}，比如一个int应该怎么存？大端序、还是小端序？浮点数呢？结构体呢？
什么是补码表示？

任何一个变量、常量都占据一定的内存区域（addr,len）。代码也是，cpu从内存中取指、解码后执行。
一个函数是一个代码端，微观上是一个指令序列，符号表。一个对象呢？也是如此。内存布局，成员函数指向函数代码的指针。
模板在编译阶段实例化，用具体类型代替。最后都转化为变量和过程。\hl{class引入了对象作用域和类作用域}。构成了符号的层次结构。
可以假设最外层有一个global namespace，作为ns树的根。

把内存看做一维向量空间，就容易理解这个问题了。
最小地址单元是byte。指针运算，与数据类型有关，void *以byte为单元。

磁盘空间也是一维向量空间，分区、文件系统、文件就构成了层次结构，靠mapping建立虚拟地址到物理地址的映射，完成\hl{存储虚拟化}的功能。
RAID/LVM都是映射，需要\hl{计算或metadata}去实现这种映射。

snapshot、LSV也是meta+data，通过引入mapping实现特性。

\hrulefill

梳理lich中用到的数据结构和算法
\begin{enumbox}
\item array
\item list
\item stack
\item queue
\item heap
\item ***
\item bitmap
\item string
\item set
\item ***
\item skiplist
\item hash
\item tree
\item graph
\item ***
\item token bucket and leaky bucket
\end{enumbox}

\hl{key的结构和分布}非常重要。索引结构不仅要考虑内存结构，还有考虑磁盘结构。
一个线性空间中key的分布有规律可循，且有序。如果key是string，则分布是多样的。

为什么数据库的索引结构用hash和B+，而不是别的？

\hrulefill

bitmap也是用来管理1D线性空间，如磁盘空间管理，没有引入中间结构，key是offset。
bloom filter是一种特殊的存在性查询结构，不能用来检索,key是字符串。

页表、inode address space，都是trie（radix tree）结构。与邮政编码、url、dns域名类似。
name是有结构的，构成prefix tree。

trie与hash、RBtree、B+皆不同，它的key是有结构的，从而形成一定的层次。用来检索一个稀疏的线性空间。
lich volume即采用了该索引结构，定为三层。路由表采用radix tree设计。

array and list是基础存储结构，其它结构都是建立在此二者之上。

\hrulefill

以上是基础数据结构和算法，加上\hl{cc、事务、分布式、机器学习}等要求后，演化出更加多彩的算法世界。
分布式算法更多是交互协议设计。

\hl{墨菲法则}，事情总比想象的要坏，真的不能再嘚瑟，认真考虑下一站是当务之急，损失什么的可以不用考虑太多。
重要的是，把握当下，面向未来。

可以参考的开源软件：
\begin{enumbox}
\item nginx
\item memcached
\item redis
\item mongodb
\item ***
\item LevelDB
\item RocksDB
\item Ceph
\item Sheepdog
\item ***
\item SPDK
\item DPDK
\item RDMA
\end{enumbox}

\hrulefill

由设备文件，可以\hl{分区、格式化、mount}。NVMe采用pci号实现kernel bypass。
读写设备文件和普通文件，采用同一套接口和语义。

io路径上\hl{page和block dismatch}，page cache和buffer cache，buffer用指针引用page的区域。
故\hl{不经过page cache的io需要block对齐，包括direct io和aio}。

bcache? \hl{1+N虚拟出N个设备}。

bit + context，要深刻理解这一点。

\hrulefill

调度器：cpu调度器和io调度。多队列。

分配器：内存allocator和fs 空间管理，\hl{命名、映射}。

定位包括cache和replication。流动的数据。

\subsection{15}

\hl{行路难，行路难，多歧路，今安在}？好好沉淀一下吧，路真的不好走，一不留神就万劫不复。

忠于专业，这是安身立命之本。进一步的学习计划：
\begin{enumbox}
\item 数据结构和算法
\item software pipelines
\item SEDA and actor
\item lambda
\item queueing theory
\item network flow
\item *** 系统
\item \hl{操作系统（包括体系结构、OS和编程语言）}
\item 文件系统
\item 网络
\item 分布式系统
\item 数据库系统
\item *** 应用
\end{enumbox}

从\hl{scheduler、allocator和cache}几个维度去研究。

先研究scheduler，有\hl{线程调度、协程调度、io调度、事务调度器}，甚至k8s一样的全局资源调度。
比如erlang、go的调度也很有特点。

\hl{任务状态，任务队列}。引入任务优先级，就是多队列。进程调度的任务就是选出下一个要运行的进程、线程。

任务上下文切换，scheduler和tasks之间是1+N的关系，交错执行。

调度分抢占式和非抢占式。线程相对于协程，多了中断方式的切换，包括硬中断和软中断。

\hl{调度器就是software pipelines里的分配器}，影响系统整体性能。

scheduler采用什么数据结构来组织任务队列呢？

scheduler采用什么QoS策略？

多调度器下的任务漂移/窃取

调度器自身是不能有长时的block任务的，对这类任务采用异步方式处理。
线程调度是抢占式，\hl{协程任务是非抢占式，要杜绝出现block操作}，由CQ来完成。

oracle asm架构

三层：\hl{并行、polling、事件处理}，并行线程的cache、memory、pci bus要local。
尽量避免并发访问共享资源。share nothing是最好的，但有时共享、通信难以避免。

polling和事件处理研究单个scheduler的情况，并行研究多个scheduler的情况。

\hrulefill

所谓异步就是不等待任务处理结果，当任务完成时，会触发事件，进入cq。

\hl{以FusionStor为例}，io有两种情况，\hl{普通disk方式和NVMe方式}。
普通disk采用aio，NVMe采用其自身的异步机制。

网络也有两种方式：TCP and RDMA。采用epoll多路机制，polling请求或完成事件。

\hl{RDMA和NVMe都是基于事件和多队列}的异步通信方式。

NVMe也很简单，用pci打开后，可以异步读写，\hl{poll时执行callback}。
Lich里主要处理了core线程和buffer的适配性。

NVMe没有使用aio线程，自身的异步机制，与coroutine结合很好。
aio还需要结合thread+epoll机制。

allocate时考虑数据块与disk的对应关系。
NVMe disk需要考虑NUMA亲和性？
普通盘连接南桥，不需要考虑？

sqlite3、redis相关的block操作，用工作线程pool。\hl{不同于aio，没有CQ这种机制}。
可以聚合起来进行批处理。因为缺少异步原生支持，可以称之为模拟aio。

网络io怎么做的？\hl{TCP和RDMA不同}，RDMA是异步机制，TCP用得是recv和send。

统一的polling框架，包括aio and NVMe，TCP and RDMA，scheduler等。
通过eventfd block到一个block点上。

\hrulefill

状态机相对协程，是否更高效？对象的队列是少不了的，也需要schedule。
每个对象的状态与task不同，需要按每种类型进行分解。而task则与进程一样，有三种基本状态。

\hrulefill

\hl{总结性能优化方法}：NUMA和CPU亲和性，Hugepage，并行、流水线、聚合等方法。
locality考虑。

virtio, IOMMU，安装在client端，感知虚拟化。

\hl{硬件虚拟化}是指处理器支持虚拟化相关指令集，
\hl{VAAI}扩展了SCSI语义，加速数据copy速度，与sendfile类似。

\subsection{16}

每个逻辑Core有一个调度队列（RBtree），所谓NUMA和CPU亲和性，就是通过mask，指定线程可以进入的队列。
调度器是一个函数，从队列或多队列里取出一个或多个要处理的元素。调度器有控制线程，控制线程可以做其它的事情。
从这个角度说，把调度器理解为一个函数为合适。线程调度、io调度和事务调度，都是这种方式。

调度包括队列、调度entity和class。用class定制调度策略。

LICH中的coroutine调度，采用了简单的FIFO方式，有优化空间。

\hl{从底层物理资源去考虑}，kernel起到资源管理的作用，即资源虚拟化。cpu、memory、devices。理解devices更复杂，
接入bus，被scan到，加载driver，然后就可以通过一定的方式进行通信，有外部事件时也可以中断方式通知cpu。
中断通知，必然是先准备好上下文。这个上下文的切换是个重点。

block用物理disk作为backing device，就是\hl{利用disk的物理特性}，而不是虚拟分区。

\hl{时间片，优先级，可抢占}是内核线程调度的几大特征。

协程与状态机方式，都需要分两层看。调度器和状态推进。状态机选出下一个要处理的任务，执行任务状态机一步或多步后退出。

调度器相当于软件pipelines中的分配器。按epoll，需要把fd加入epoll的兴趣列表，
同时维护分离表/routor：fd到node(event handler以及context)的映射关系。

RDMA用了两层分离机制：epoll用于连接管理，自身的polling机制。

\subsection{17}

\hl{在lich里引入pg如何}？object-pg-osd的关系由两级映射完成，一是降低了metadata量，二是恢复平衡更容易做。
这融合了ceph和lich两者的优点。

如osd故障，pg处在降级状态。与lich引入vfm要解决的问题类似。\hl{引入最小副本数}，会使得方案更优雅。
从osd故障到数据恢复，可以定制恢复策略，如时长等。

lich的chunk和replication，类似于RAID的条带化和mirroring。

传统盘阵的控制器结构，有双控、多控。双控关系有A/A，A/P。横向扩展。LUN的归属。
\hl{理解MPIO在多控架构下的工作原理}。

单处理器的false sharing。\hl{同一cache line}上有多个数据项，被不同线程更新。

多处理器的缓存一致性（同一存储位置），存储一致性（不同存储位置）。NAS的client 缓存一致性

ABA问题

本地文件系统
\begin{enumbox}
\item \hl{RAID/LVM}
\item ext2/3/4
\item XFS
\item ZFS
\item btrfs
\item ReiserFS
\item ***
\item Fuse
\end{enumbox}

分布式文件系统
\begin{enumbox}
\item GFS/HDFS
\item GlusterFS
\item Lustre
\end{enumbox}

与ceph对比
\begin{enumbox}
\item 数据分布(元数据管理)
\item 副本分布(节点为最小故障域粒度)
\item TP
\item 卷控（无日志，顺序一致性语义）
\item 恢复、平衡、流控
\item ***
\item \hl{RDMA/DPDK/SPDK}
\item \hl{iSER/NVMf}
\item Cache
\item Tier
\item ***
\item \hl{Snapshot}
\item Remote Replication
\item ***
\item EC
\item Dedup/Compress
\end{enumbox}

Lich优化项
\begin{enumbox}
\item pool下flat namespace，不需要支持多级目录
\item 引入类似pg的结构，二级映射
\item Node内管理磁盘，而没有把disk暴露
\item ***
\item 卷控平衡
\item 查找卷控位置采用了multicast机制，为何不去admin上根据lease情况找？
\item 数据访问需要vctl中转，client不能存取chunk (与array的双控架构做对比)
\item 卷控拆分成子卷，支持大容量卷
\item 恢复可以由卷控自己执行(智能卷控)
\item 停止恢复
\end{enumbox}

\hrulefill

Amazon EBS挂载到一个EC2实例上，EBS可以打快照，快照保存到S3上。
第一个快照是全量的，后续快照是增量的。

对一个pool，容错级别与副本数有关。故障域要大于等于副本数。\hl{EC能改进这个问题}。
降低同等或更高容错等级下的成本。
同等品质的硬件配置，pool越大，硬件发生故障的概率就越高。所以pool的规模也有上限。

ceph数据一致性检测和恢复是以PG为基本单元进行的。PG是对象的集合，PG对应的OSD存储了对象的副本。
副本一致性检测：\hl{peering, recovery, backfill, scrub}等过程。

PG上所有对象的写入，有primary OSD调度，串行化并记录有日志。

故障情况下，OSD(include primary and replica OSD)出现降级对象。

\hrulefill

RAID、副本、EC放在一块去理解。RAID有条带化、镜像、RAID5，在分布式系统中，对应\hl{分块，每块数据做副本orEC等机制}。
一是数据分块，放到多个节点上，提升并行度。再一个是容错等级，副本、EC是两种标准方式。
\hl{副本一致性和条带一致性是难点}。副本一致性通过RAFT协议去保障。

scrub可以发现静默错误，依赖的是checksum。

存储处在新旧之交，在快速演进。硬件和软件架构都在变革之中。
硬件引入了RDMA、SPDK、DPDK、NVMe等快速网络和存储介质、协议。
软件架构从传统阵列的SCSI、SAN、NAS到新的分布式架构。

新型存储SDS，包括Ceph、Amazon EC2、阿里的盘古等。

\hrulefill

快照，从COW到ROW到LSV，ROW用户数据共享，LSV不仅用户数据共享，而且数据出现多版本，所以需要GC。

ROW2和ROW3，都是带两级bitmap index。不同在于ROW2在新写时，按chunk发生了COW，
ROW3在新写入时不发生COW，但导致更严重的碎片化，影响读性能。

写有对齐写、不对齐写、overwrite覆盖写。读有对齐读和不对齐读。\hl{不对齐的写和读需要特别处理}。

\hrulefill

\hl{按对象、块、文件遍历本地和分布式系统}，更复杂的NoSQL、NewSQL，暂时没有精力照顾到，后期也需要整理。
从哪些维度着眼？\hl{分布、副本/EC、sharding、快照}等。

先有disk，再有fs，fs里继续有disk（设备文件），我中有你，你中有我的关系。

怎么理解object、block、file之间的关系？\hl{NAS和SAN}在实际的应用场景中，有什么特别之处？
大家又是怎么使用的？

\hrulefill

\hl{纸上得来终觉浅，绝知此事要躬行}。看书、源码的时间要慢慢过渡到分析问题、解决问题。
头脑中要有big picture，在此基础上深化。如果没有这个，很容易迷失在技术的细枝末节里无力自拔。

更基础的知识放在长期的学习计划里，定量有恒，日积月累，定有所获。

\hl{戒定慧，此三学六度}，实在是度人的船阀，还有很深刻简练的吗？
故称三无漏学，其道甚大，百物不废。何必舍近求远？实乃没有甚深定力和慧解的表现。

应渐渐地少看书了，多动脑、多动手，在解决问题的过程中，扩展知识和能力的边界。
(这段描述，有所不为而后可以有为，然后有所得) 知止是高级智慧。

AFA是方向，相关技术包括\hl{RDMA/DPDK/SPDK，NVMe、iSER、NVMf}等。
这些方面应该是下一步的重中之重，道不远人，如果舍近求远，恐有缘木求鱼的危害。

\subsection{18}

cache一致性与副本一致性存在很大的不同。cache一致性出现在共享内存多处理器、分布式文件系统client cache等场景。
副本一致性出现在replication和EC等场景。

cache一致性关注的是一个数据源，多个cache时的行为。副本一致性表示多个副本之间的一致性。
cache一致性可以用lease、oplock，可靠多播，MESI协议解决，副本一致性用RAFT协议解决。

\hl{完善性能分析和故障诊断toolbox}。

按\hl{主体-对象模型}，一致性可分为观测一致性和数据一致性。

\hl{CAP和ACID}，深入理解种种细分情形。

\hrulefill

lich meta里记录的是chkid到nids的映射，每个nid通过db查询到实际的disk loc。
相当于两级映射。

\hl{chunk tree过于复杂}，难以做到事务性的要求。
allocate的时候，有可能会连续更新多级chunk。
迁移或故障的时候，也会引发级联更新。

lich无日志，如何保证一致性的？

\hl{lich chunk内并发}，chunk内非覆盖区域，clock的连续性，理论上可以或并发或批量写入。
有一次排序、精简、聚合、并发的机会。覆盖区域，按fifo顺序提交。
如clock不连续，则需要等待。（\hl{paxos的日志写入，放松了该要求}）

每个term包括\hl{选举、恢复、正常操作}诸阶段。

\hl{如何减少故障下的io中断时长}？admin、vctl可能发生切换、或reload。需要case by case分析。

实现LSV时采用了该方案。\hl{如果io size不规则，当如何}？
log和bitmap的写入，采用了pipeline模式。以确保log的并发提交，bitmap的顺序提交。

rcache比较复杂，开始按不同size建立多个cache是不明智的，应统一采用page cache，易于管理。
不过可以加入readahead等策略。

EC/dedup/compress/都需要建立新的映射。share底层数据块，但造成严重的碎片，对顺序读不友好。

\hl{回顾open vstorage的映射关系}。zerocopy snapshot？索引、共享、多版本？
多版本比共享有更复杂的引用关系。

\hrulefill

建立知识体系，依次scan各个分支领域。
\begin{enumbox}
\item \hl{CAP consensus, include paxos, RAFT and lease}
\item ACID transaction ARIES
\item metadata管理，如GFS、BigTable等。
\end{enumbox}

\hl{这次彻底把Paxos研究透彻，不能在一个地方跌倒两次。
否则就是有勇无谋，把握不住重点所在}。

戒定慧三字深邃，作为下一阶段的座右铭。

\subsection{19}

庄子内七篇、六祖坛经，从专业的角度去解读，很有意味。工匠精神、企业家精神是一而二、二而一。
回归专业是正确选择，一直知道、一直不能很好执行，知行不能合一。这次不能再偏离。

从分布式存储开始，建立问题和知识联动的体系，以助力其后的职业发展。

\subsection{21}

怎么查看cs和中断信息？

如何处理slow disk？

性能之颠：\hl{可观测指标=f（资源，workload）}。资源有使用率，基于时间(排队论？)、或容量。
在software pipelines里，workload是上游，可观测指标是下游，架构影响性能(化学)。

资源：\hl{CPU,memory,fs,disk and network}。

多网卡，MPIO。

面试经验谈
\begin{enumbox}
\item BAT，TMD，浪潮，联想等。
\item 仔细整理做过的东西，要更深入。不熟悉的东西宁可不写。要非常严谨
\item 用的什么型号的设备，测试的性能。
\item RDMA是重点，建立连接的流程，遇到的坑。
\item 基础编程题，本质上都是工程师
\item 要知道lich的不足之处
\end{enumbox}

引入kv用于元数据管理，加强client功能，client可以与object直接通信，更大的chunksize。
rich client，目前vctl就好像是这样的rich client。

恢复和数据平衡都依赖于控制器平衡，才能有更好的并发度？

\subsection{22}

CAP平衡ACID和BASE，进而引入单副本一致性问题，RSM的解决引入paxos、raft。

