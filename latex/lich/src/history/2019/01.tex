\chapter{2019}

\section{01}

\subsection{08}

用本文档记录日志，只维护一个即可。growth记录更高层次的指导原则，一般来说相对比较稳定。
如此就形成了一个相互促进的双环架构。

不要自我设限，一定如何如何？打基础，看机遇。

一些指导原则
\begin{enumbox}
\item 概念图、重要的是理解核心概念，费曼学习法，用自己的语言描述出来，取象比类，借助类比、联想等思维方式。
\item 在默认模式/执行模式，走神/专注等进行切换，\hl{文武之道、一张一弛}，有助于激发出创造力。
\item 强调综合判断能力。
\item 慢练
\end{enumbox}

体系架构、操作系统和汇编语言是底层逻辑。数据结构、算法、编程语言结合起来进行学习。

坚持专业技术方面的发展，\hl{CBA都要有所涉猎}。现在正是最佳时机，不能再没有重点了。

重点应放在做过的项目和全闪上。\hl{多路径？多存储网段？SPDK、NVMe}等。
比如单机存储引擎，用sqlite3、redis、RocksDB，为什么？

从势、道、法、术、器等维度进行梳理。

\subsection{09}

clone后，没把源快照置为protected状态。

\hl{software pipelines和排队论是性能分析利器}。SEDA、actor和lambda是架构方法。
图论、网络流，老三论。按中医理论，人体由藏象经络组成，运行气血。与网络系统有很强的相似性。
图是最复杂的数据结构，降维去看其它数据结构会如何？petri net和有限状态机。
把以前生活中经历的点连成线，会更明晰。

专业的力量、能力变现、价值规律、写作是最好的自我投资、自我进化、放大核心优势都是很好的理念，关键在落实。

本质上是核模式的应用。先在一个小的专业领域建立根据地，再顺势裂变扩张。

眼观六路耳听八方，行业格局分析至关重要，技术加商业两条腿走路。

自己要变得强大，这是积极心态，接下来是如何才能变得强大，这是现实，
要做现实的造梦者，不做桃花源中人。

lich的io 路径需要控制器中转，是否影响性能？client直接读写数据会如何？ceph client与osd primary直接通信。

只有一个RDMA网卡，存在硬件上限。如何使用多个RDMA网卡呢？8k 100w iops。

接触斯多葛主义。内心的自由和平静。

Oracle asm?

\subsection{10}

向身边的人学习，虚心，不要老端着，自认为了不起，其实需要理解的东西很多。
差不多先生，含混不清是极大的弊端和缺陷，一定要清晰，清晰才有力量。
表达要简洁清澈清晰。空性不代表含混，而是包容。

写作是最好的自我投资，要深刻理解这句话，并每日打磨写作技能。
先记下流水账，固定时间进行归纳整理，提炼升华。

做事情太保守，\hl{视野不够高，格局不够大}，现在呢？更多自以为是，
需要沉下心来，好好观察、琢磨。看清趋势。正心诚意，取势、明道、优术，然后是利器、举例。

集中一段时间，专攻技术。围绕关键问题，从广度和深度，\hl{致广大而尽精微}。
技术综述好做，选几本书多读几遍即可，难的还是深度。另外一条途径，就是多与人聊技术。

围绕全闪进行写作。全力以赴吧。

\hrulefill

\hl{重点整理在HY做过的工作}：
\begin{enumbox}
\item snapshot and LSV
\item Recovery
\item Balance
\item QoS
\item ***
\item ETCD
\item Network(TCP and RDMA)
\item ***
\item Scheduler
\item Memory Allocator
\item Disk Management (Local Storage Engine, +NVMe)
\item ***
\item iSCSI/iSER/NVMf
\end{enumbox}

\hl{恢复、平衡、删卷/snapshot、rollback、clone、flat}等操作，如何用统一的任务管理系统进行管理。
可以借鉴k8s等集群管理系统的经验。有了总控，可以更好地加入策略。

\hl{恢复和平衡}有些逻辑是通用的，如按pool处理，检测本地vctl变化，扫描本地vctl。
可以放到一个处理框架内进行处理。ceph如何进行数据平衡呢？

太大的调度粒度(vctl)不利于任务调度和负载均衡。qos如同节流阀。可以放到不同的位置上。

\hl{if-what}当前的恢复处理逻辑，如果发生\hl{网络分区故障}，会如何？

为了支持多网卡，需要做什么？

为了支持多路径，需要做什么？

\hrulefill

研读google发表的系统相关论文，看真实需求以及技术演进方向。
性能、稳定可靠、容错高可用、可扩展是几大非功能属性，决定产品品质。

理解google的技术体系，不能老是泛泛而谈。borg衍生出k8s。
\hl{hdfs、bigtable、megastore、mapreduce}都已经实现了容器化和统一管理。

有bigtable的sstable演化出leveldb和rocksdb。redis可以与rocksdb进行组合，改变全内存数据库的局限性。

催生了hadoop生态，进一步衍生了spark等lambda架构的统一计算平台。

\hrulefill

光点图灵

RAID之上有LVM，都是单机环境下的产物。虚拟化程序越来越高。进而是盘阵，然后是分布式架构,
scale-out越来越强，灵活性越来越高，软件定义和超融合由此而来。

MongoDB直接做RAID0，然后利用其自身的集群能力，master/slave replication架构，
可以failover and failback。这相当于内置存储。当时对存储层面的理解相当有限。

对gridfs扩容是如何做的？通过多个目录做的，也不设计磁盘虚拟化方面，如LVM。
面对的就是一个扩展性问题。

\hl{迁入阿里云、采用七牛对象存储后}，不自己维护服务器，避免了很大的麻烦。

\hrulefill

\hl{美地森的工作和成长经历}：

很长时间都理解不离aio、事件驱动的实质所在。
做技术确实死磕精神，重视贪多求大，结果反而欲速不达。

当时做edog又是如何做的呢？有什么可以反思的？
集群管理采用了Erlang，通过libvirt对接qemu，改了qemu的驱动，接入yfs作为底层存储。

对网络理解不够透彻，接触也少。前端时间获取交换机管理信息，技术收到都是类似的。
不过关于交换机，很多技术细节理解不到位。\hl{天马行空惯了，细节把握不到位}。
需要改进，做技术要认真再认真，程序化，每日记录、反思、总结、提炼。
既要有广度，深度更是必不可少。\hl{专精一件事，就可以立于不败之地}。
找到了自己的哲学、守中致和，核模式，就要学以致用，一以贯之，用来指导自己的言行。

接触了hadoop，为什么没有坚持呢？放着这么好的东西，不去跟进，
只是习惯于做些乱七八糟的东西，实在是有眼无珠呀。

虽然也有跟进，\hl{不坚定，不深入}，当然更谈不上真的掌握。

\hrulefill

华胜天成：法国电信做手机的SIP协议，管理软件，一头雾水。

包括以前在\hl{天地伟业、鸿业科技}，都没有深入业务，总是局限在一个小小的技术视角。
现在软硬件的变化，真是日新月异，不进则退。\hl{没有一个好的平台和机遇}，就很难崛起。
如何把握好的平台和机遇，当然要靠本日一点一滴的积累，磨炼基本功，把握大趋势。

鸿业科技做的\hl{官网计算}回忆起来，倒是给了很好的启示，说明了管道理论的普遍性。

一是个人成长放在首位，接着就是睁大眼睛，静静地坚定地去找团队、平台和机遇。（\hl{人和事的胜任度}）
两者匹配了，能力胜任度高，自然一切水到渠成。所以也不必着急焦虑，反反复复磨炼自己就够了，
金子总会发光的，好酒也怕巷子深。这些看似对立的格言，其实包含了深刻智慧。

这几年，技术发展真是太快了。风口一个也没把握住，原因何在呢？

\hl{接下来干什么呢}？

转移关注点，放低姿态，学习关键知识点，并记录在案。从整体系统中抽离，聚焦于更小的模块。

最近老王他们进行的性能优化工作是值得关注的架构调整。支持多路径，多网卡，甚至插到不同的交换机port上对性能都有影响(8x or 16x)，以提升性能。
性能的理论上限如何预估？

更多地转变到SPDK(NVMe, NVMf)、RDMA上面来。全NVMe方案如何？怎么构建全局共享，client与数据不同于控制器直接存取？
可以参考的资源有什么？

纪翠叶问及fio的部分参数什么含义？这是器层面的知识点。

\subsection{11}

中庸是我的圣经。从心道物等诸多维度确定了必要的理念。

\hrulefill

不深入理解传统存储的高度和局限，就不能理解很多核心概念，比如多路径、共享、全局缓存等。
分布式架构在\hl{扩展性、灵活性}方面胜出。

通过真假latency，可以估计等效并发。方法如下，单并发测量固定时延。多并发测量iops，取倒数即为假时延。
固定时延和假时延的比值就是该配置下的等效并发度。

io path也有串行和并行两部分，从而引入加速比的概念，上面的等效并发度就是加速比。
可以评估整个体系结构的并行执行效果。

按pipelines理解体系结构，体系结构也是网中网的分形结构。
官网、电路等等可以作为分析模型，从流体动力学和物理学里吸收能量。
但也不必把问题复杂化。从最少的元素推导出更多的规则，解释更多的现象，是科学的研究方法。

从体系结构、操作系统的原理和知识，去深入存储子系统的相关理解。

\hrulefill

\hl{网络分区}下，恢复、平衡和一般的IO是如何进行的？

比如一个5节点集群，采用2副本策略。分区意味着什么呢？怎么理解多数派这一要求。
5节点2副本，也是只能容忍单个节点故障，\hl{故障容错度是有副本数决定的}，与集群规模无关。
比如一个10节点的集群，2副本的情况下也只能容忍一个节点故障？

可以通过划分故障域改进这一点。副本数据分布到不同的故障域，容错按故障域来定义。

保护域和pool是等价的概念，保护域是一种物理隔离机制。把一个大集群分拆成小集群。所以基本的嵌套关系是\hl{集群-保护域-故障域-节点-磁盘}。

网络的VLAN是否也是一样的分区机制？

这一点与zk、etcd等协同共识系统存在很大的不同。

MySQL master/slave架构，能否自动切换？

\hrulefill

client直接与数据块通信，lich需要通过vctl中转，如何克服这一点？

多网卡支持，通过多网卡连接控制器，MP软件用来管理导出的LUN，确保只有一个。

SSD FTL中心任务是映射管理，与OS的页表、LSV的快照管理是一样的，关键是维护一个虚拟地址到物理地址的映射。
可以是一级，也可以是多级。\hl{伟大的计算原理}里面，讨论存储的时候提到四点：\hl{命名，映射，定位，授权}。
可以按此四个维度去思考林林总总的存储系统。

\hl{按主体-对象模型，授权有两种}：CL和ACL。CL以主体为中心，ACL以对象为中心（如文件系统的ACL）。

\hl{全力投入AFA}，大量阅读，深度思考。知识就是力量，这点认识还是不够深刻。

地址空间管理，抽象成了一切皆文件。内存、磁盘、文件都是地址空间。文件系统的设计需要充分地认识到这一点。
SSD最核心的功能也是这一点。SSD FTL很类似于LSV，很多可类比的地方，如映射、GC。

数据结构主要也是映射，如hash、map、graph，graph是最通用的映射关系。
再如，函数也是映射。lambda架构也提及此，map-reduce更成了并行计算模式之一。

\hrulefill

多快好省，快和好是当前重点，多和省在此基础上，迭代优化。\hl{性能、负载、高可用}是几种集群形式，
bonding，多网卡支持。bonding具有什么特征？只是HA？而不能同时工作，聚合性能和均分负载？

\subsection{14}

\hl{化繁为简，打破学科边界}，从实际问题及其方案开始理解。深入理解计算机系统即是采用这种叙述法。

计算机系统可分为cpu(\hl{进程、并发、中断、隔离})、memory、io子系统(storage/fs and network)等，按此结构循序展开。
io又分存储和网络等，组成tree架构。关键组件：\hl{调度器(thread、io)，空间分配器（cache、memory、disk）}。

采用流一元论(flow dynamics)的描述框架，cpu居中调度，上下文切换和内存copy，可称为指令分配器。
\hl{flow由节点和边组成}，节点是处理单元或节流阀，边是管道。处理方式可以有多种：转发、映射、压缩、消重等。

衡量流效率的指标是latency、iops、throughput等。流的平衡态由平衡方程描述，输入过多，会导致处理不过来，
发生拥塞、溢出等故障，故需要限流/节流。

memory copy消耗总线带宽，bus是如何利用的？

先理解一般case，加入cc和故障恢复，系统就变得复杂了。虚拟化、cc、持久性是操作系统研究的三大块。

cpu亲和性需要通盘考虑，不仅仅是内存，还有disk、network，读设计线程和buffer，是否与core线程处于一个NUMA节点？
还是跨NUMA节点进行内存copy？ aio线程尽量与core线程距离近。\hl{如果只要一块网卡，则网卡属于哪个NUMA节点呢}？

\hrulefill

\hl{数据结构的内存layout}，比如一个int应该怎么存？大端序、还是小端序？浮点数呢？结构体呢？
什么是补码表示？

任何一个变量、常量都占据一定的内存区域（addr,len）。代码也是，cpu从内存中取指、解码后执行。
一个函数是一个代码端，微观上是一个指令序列，符号表。一个对象呢？也是如此。内存布局，成员函数指向函数代码的指针。
模板在编译阶段实例化，用具体类型代替。最后都转化为变量和过程。\hl{class引入了对象作用域和类作用域}。构成了符号的层次结构。
可以假设最外层有一个global namespace，作为ns树的根。

把内存看做一维向量空间，就容易理解这个问题了。
最小地址单元是byte。指针运算，与数据类型有关，void *以byte为单元。

磁盘空间也是一维向量空间，分区、文件系统、文件就构成了层次结构，靠mapping建立虚拟地址到物理地址的映射，完成\hl{存储虚拟化}的功能。
RAID/LVM都是映射，需要\hl{计算或metadata}去实现这种映射。

snapshot、LSV也是meta+data，通过引入mapping实现特性。

\hrulefill

梳理lich中用到的数据结构和算法
\begin{enumbox}
\item array
\item list
\item stack
\item queue
\item heap
\item ***
\item bitmap
\item string
\item set
\item ***
\item skiplist
\item hash
\item tree
\item graph
\item ***
\item token bucket and leaky bucket
\end{enumbox}

\hl{key的结构和分布}非常重要。索引结构不仅要考虑内存结构，还有考虑磁盘结构。
一个线性空间中key的分布有规律可循，且有序。如果key是string，则分布是多样的。

为什么数据库的索引结构用hash和B+，而不是别的？

\hrulefill

bitmap也是用来管理1D线性空间，如磁盘空间管理，没有引入中间结构，key是offset。
bloom filter是一种特殊的存在性查询结构，不能用来检索,key是字符串。

页表、inode address space，都是trie（radix tree）结构。与邮政编码、url、dns域名类似。
name是有结构的，构成prefix tree。

trie与hash、RBtree、B+皆不同，它的key是有结构的，从而形成一定的层次。用来检索一个稀疏的线性空间。
lich volume即采用了该索引结构，定为三层。路由表采用radix tree设计。

array and list是基础存储结构，其它结构都是建立在此二者之上。

\hrulefill

以上是基础数据结构和算法，加上\hl{cc、事务、分布式、机器学习}等要求后，演化出更加多彩的算法世界。
分布式算法更多是交互协议设计。

\hl{墨菲法则}，事情总比想象的要坏，真的不能再嘚瑟，认真考虑下一站是当务之急，损失什么的可以不用考虑太多。
重要的是，把握当下，面向未来。

可以参考的开源软件：
\begin{enumbox}
\item nginx
\item memcached
\item redis
\item mongodb
\item ***
\item LevelDB
\item RocksDB
\item Ceph
\item Sheepdog
\item ***
\item SPDK
\item DPDK
\item RDMA
\end{enumbox}

\hrulefill

由设备文件，可以\hl{分区、格式化、mount}。NVMe采用pci号实现kernel bypass。
读写设备文件和普通文件，采用同一套接口和语义。

io路径上\hl{page和block dismatch}，page cache和buffer cache，buffer用指针引用page的区域。
故\hl{不经过page cache的io需要block对齐，包括direct io和aio}。

bcache? \hl{1+N虚拟出N个设备}。

bit + context，要深刻理解这一点。

\hrulefill

调度器：cpu调度器和io调度。多队列。

分配器：内存allocator和fs 空间管理，\hl{命名、映射}。

定位包括cache和replication。流动的数据。

\subsection{15}

\hl{行路难，行路难，多歧路，今安在}？好好沉淀一下吧，路真的不好走，一不留神就万劫不复。

忠于专业，这是安身立命之本。进一步的学习计划：
\begin{enumbox}
\item 数据结构和算法
\item software pipelines
\item SEDA and actor
\item lambda
\item queueing theory
\item network flow
\item *** 系统
\item \hl{操作系统（包括体系结构、OS和编程语言）}
\item 文件系统
\item 网络
\item 分布式系统
\item 数据库系统
\item *** 应用
\end{enumbox}

从\hl{scheduler、allocator和cache}几个维度去研究。

先研究scheduler，有\hl{线程调度、协程调度、io调度、事务调度器}，甚至k8s一样的全局资源调度。
比如erlang、go的调度也很有特点。

\hl{任务状态，任务队列}。引入任务优先级，就是多队列。进程调度的任务就是选出下一个要运行的进程、线程。

任务上下文切换，scheduler和tasks之间是1+N的关系，交错执行。

调度分抢占式和非抢占式。线程相对于协程，多了中断方式的切换，包括硬中断和软中断。

\hl{调度器就是software pipelines里的分配器}，影响系统整体性能。

scheduler采用什么数据结构来组织任务队列呢？

scheduler采用什么QoS策略？

多调度器下的任务漂移/窃取

调度器自身是不能有长时的block任务的，对这类任务采用异步方式处理。
线程调度是抢占式，\hl{协程任务是非抢占式，要杜绝出现block操作}，由CQ来完成。

oracle asm架构

三层：\hl{并行、polling、事件处理}，并行线程的cache、memory、pci bus要local。
尽量避免并发访问共享资源。share nothing是最好的，但有时共享、通信难以避免。

polling和事件处理研究单个scheduler的情况，并行研究多个scheduler的情况。

\hrulefill

所谓异步就是不等待任务处理结果，当任务完成时，会触发事件，进入cq。

\hl{以FusionStor为例}，io有两种情况，\hl{普通disk方式和NVMe方式}。
普通disk采用aio，NVMe采用其自身的异步机制。

网络也有两种方式：TCP and RDMA。采用epoll多路机制，polling请求或完成事件。

\hl{RDMA和NVMe都是基于事件和多队列}的异步通信方式。

NVMe也很简单，用pci打开后，可以异步读写，\hl{poll时执行callback}。
Lich里主要处理了core线程和buffer的适配性。

NVMe没有使用aio线程，自身的异步机制，与coroutine结合很好。
aio还需要结合thread+epoll机制。

allocate时考虑数据块与disk的对应关系。
NVMe disk需要考虑NUMA亲和性？
普通盘连接南桥，不需要考虑？

sqlite3、redis相关的block操作，用工作线程pool。\hl{不同于aio，没有CQ这种机制}。
可以聚合起来进行批处理。因为缺少异步原生支持，可以称之为模拟aio。

网络io怎么做的？\hl{TCP和RDMA不同}，RDMA是异步机制，TCP用得是recv和send。

统一的polling框架，包括aio and NVMe，TCP and RDMA，scheduler等。
通过eventfd block到一个block点上。

\hrulefill

状态机相对协程，是否更高效？对象的队列是少不了的，也需要schedule。
每个对象的状态与task不同，需要按每种类型进行分解。而task则与进程一样，有三种基本状态。

\hrulefill

\hl{总结性能优化方法}：NUMA和CPU亲和性，Hugepage，并行、流水线、聚合等方法。
locality考虑。

virtio, IOMMU，安装在client端，感知虚拟化。

\hl{硬件虚拟化}是指处理器支持虚拟化相关指令集，
\hl{VAAI}扩展了SCSI语义，加速数据copy速度，与sendfile类似。

\subsection{16}

每个逻辑Core有一个调度队列（RBtree），所谓NUMA和CPU亲和性，就是通过mask，指定线程可以进入的队列。
调度器是一个函数，从队列或多队列里取出一个或多个要处理的元素。调度器有控制线程，控制线程可以做其它的事情。
从这个角度说，把调度器理解为一个函数为合适。线程调度、io调度和事务调度，都是这种方式。

调度包括队列、调度entity和class。用class

LICH中的coroutine调度，采用了简单的FIFO方式，有优化空间。

\hl{从底层物理资源去考虑}，kernel起到资源管理的作用，即资源虚拟化。cpu、memory、devices。理解devices更复杂，
接入bus，被scan到，加载driver，然后就可以通过一定的方式进行通信，有外部事件时也可以中断方式通知cpu。
中断通知，必然是先准备好上下文。这个上下文的切换是个重点。

block用物理disk作为backing device，就是\hl{利用disk的物理特性}，而不是虚拟分区。

\hl{时间片，优先级，可抢占}是内核线程调度的几大特征。
