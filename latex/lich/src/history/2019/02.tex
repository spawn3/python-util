\section{02}

\subsection{02}

\hl{Reactive Programming与CAP关系密切}。追求的value是即时响应、可维护性和可扩展性。在正常情况下是如此，即便发生故障，或负载变化，也是如此。
如何做到？引入了async、non-blocking消息传递，引入了显式的消息队列。通过MQ，如何实现以上诸特性呢？

系统划分成组件树的成绩结构，明确职责、隔离故障，并把故障委托给独立的故障处理单元。

\hrulefill

Lich的故障处理，应该有一套框架去处理所有故障，包括磁盘、节点，甚至把balance也纳入其中。
通过发送消息来触发相应的处理逻辑。

维护显式的消息队列，放在哪儿？client端，卷控端？如放在卷控端，由谁来调度执行？
卷控实则是个非执行体，只是一个内存结构。

可以由卷控所在的core线程去调度、或采用timer的方式。

MQ的task之间，如果由依赖关系，如io读写请求，需要同步机制。

并发提交的每个请求对应一coroutine，在scheduler层面，形成了一MQ。

\hrulefill

Lich的task，与actor有很多不同，如没有mailbox，不能形成监督树。

\subsection{04}

明体达用，理论是体，在各种系统中的实现是用。整理分布式存储系统的方方面面，不能脱离具体的工程实践去抽象地理解理论。
Lich也是诸多系统之一。

按自然的演进过程，从ACID到CAP。单机事务是思考的起点，CAP以及其改进是设计原则。
加入复制后，情况变得更为复杂。

ACID中，I是隔离性，需要CC机制来保障。A/D是原子性和持久性，需要UNDO、REDO日志来保障。C是业务数据完整性约束。
与CAP中的C有所不同。CAP中的C是原子读写，如线性一致性、顺序一致性、因果一致性、会话一致性。

CAP也是数据库的设计的指导原则。\hl{CAP的最新形式是PACELC，与Reactive Programming很类似}。
分布式存储系统是现代大规模高并发网站架构的一个主要部分，按分形学，有很多共性。

线性一致性对读写操作都要执行全序原子多播。顺序一致性需要对写操作执行全序的原子多播。

一致性可以看作物演通论的生存度，一致性递弱需要相应的补偿协议，从强一致性到最终一致性是一个连续的光谱。
一致性需要高于某一阈值，系统才是健康的。一致性降低必有相应的代偿协议。

为什么要降低一致性？或为什么一致性会降低？

\hl{single master与primary copy}不同，前者针对的是真个数据集，后者针对的是某一数据项。
还有一种peer to peer系统，就是一个数据项的更新也会涉及多个primary。

两者之区分实质上也就是\hl{paxos协议中无主和有主的区分}。我们把RAFT看作paxos协议的一种实现。
通过强化约束，来简化实现的复杂度。

\hrulefill

DB引擎，意索引。先考虑CC(隔离性，事务调度器)，次考虑提交协议，最后考虑基于日志的故障恢复(A/D)。
扩展到分布式数据库系统，需进一步考虑\hl{CC和提交协议如何去满足ACID属性}。

整理数据库相关知识，包括单机、分布式数据库，从数据模型上，包括SQL/NoSQL/NewSQL。
主要从几个维度进行：
\begin{enumbox}
\item 数据分布
\item 复制
\item index
\item transaction
\item 读/写
\end{enumbox}

\hrulefill

\hl{CAS/ABA，时间戳，向量时钟}。

MVCC里对每行记录增加两个隐藏version。

除了IO之外，还有并发的其他任务，如replica cleanup。

有多种CC方法，主流的是lock，\hl{TO、MVCC都是基于版本号/timestamp}。

\hrulefill

把以上理论进一步落实到Lich和Ceph的设计中。
\begin{enumbox}
\item 更新chunk时递增info version？
\item 在snapshot也实用了version。
\end{enumbox}

\subsection{05}

同心圆模型，最核心的是设计心法，从内往外以此是硬件，软件(OS、FS、DB、PL/Compiler)，网络，分布式系统。
设计和算法是两大重点和难点，可放入最内圈。

可靠性、可用性，容错、resilient，可伸缩性、elastic，可扩展性等概念有着细微的差别。

性能\&故障诊断分析方法：
\begin{enumbox}
\item Pipeline
\item Queuing
\item ***
\item PAT
\end{enumbox}

数据存在的几种形态：
\begin{enumbox}
\item Storage
\item Cache
\item MQ (stream)
\end{enumbox}

Storage和Cache具有极大的相似性。

至于async更多是一种架构思想。Reactive Programming强调了MQ的重要性，\hl{message-driven是实现RRE的良方}。

动静分离的静，最适合应用多级cache，御敌于国门之外，最终到达源站。
无状态的业务逻辑层与有状态的数据层，如何进行伸缩是有很大不同。

\subsection{07}

数据层面：复制(EC、dedup、compress)、snapshot、cache等等。

卷的snapshot，cc中快照隔离，日志的检查点技术

\hrulefill

adesk架构演进之路:
\begin{enumbox}
\item 单库无拆分->动静分离->master/slave的replication架构。
\item 加入ElasticSearch，支持全文搜索功能。
\item 采集行为日志，进行数据分析。google analysis，umeng等。
\item 对接广告平台进行变现
\item \hl{上云}：不再自己运维。采用了阿里云和七牛的云平台。
\end{enumbox}

性能：索引，cache(CDN etc)、负载均衡

可伸缩性：动静分离、增加磁盘。

推送

监控：第三方监控、nagios、zabbix

增长黑客

\subsection{17}

进入刷题模式，有的放矢、定量有恒地刻意练习，以刻意练习的理念概括之。

由CAP定量出发，来理解分布式系统的复杂性。CAP/PACELC过渡到Reactive Manifesto。

pipeline、SEDA、lambda，软件管道三原则。

分布式一致性至关重要，相关协议有:
\begin{enumbox}
\item 2PC/3PC
\item paxos
\item pacificA
\item raft
\end{enumbox}

可以数据库的事务管理来类比：\hl{cc和故障恢复}。cc有2pl和基于时间戳的多版本机制。
故障恢复基于log/journal，支持redo和undo操作。

ACID的A/D关乎故障恢复，I关乎并发控制。事务的隔离级别，最少要read committed。因为可以undo，不加控制的话，会出现read uncommitted。

提交不变性：

最大提交原则：提交的一定满足A/D，未提交的如何处理？

所谓顺序一致性就是可以交叉，但要看到共同的交叉顺序。\hl{意味着并不需要UNDO操作？只要原子写入}？
\hl{顺序一致性}，lich支持顺序一致性，可能读到uncommitted数据，由上层应用来保障完整性。

分布式计算：在分布式环境中，强一致性因全局时钟限制很难实现。程序员开始寻找条件不那么严苛的模型，
lamport利用逻辑时钟代替全局时钟提出顺序一致性，其定义如下：
\begin{enumbox}
\item 所有执行结果相同，就像处理器的所有操作是在一定的顺序下执行的一样。
\item 在本地时钟下，本地操作顺序为本地程序的执行顺序。
\end{enumbox}

不同处理器之间的操作出现重叠是可能的，但顺序一致性要求所有处理器看得的重叠相同。

paxos的二阶段可用温故知新概括之。

membership change比较复杂，joint联合共治，多数派原则？

\hrulefill

快照：COW、ROW 1/2/3、LSV。

快照不变式，由快照读定义。

先理解当前实现的COW快照树。主要问题有几个：
\begin{enumbox}
\item 维护快照树
\item 卷和快照如何组织，与底层裸卷是什么关系？1:1，还是n：1?
\item 快照头有无完整索引？索引采用什么结构？
\item 如果有索引，索引的管理粒度是多大？page？
\item 如何保障\hl{读的不变性}？ how to read
\item 每种操作的性能和复杂度
\item 支持的快照数量
\end{enumbox}

分析写和读过程。新create的快照无数据，后续也只是存储私有数据。
所以没有完整索引，会有读(快照读，而不是卷读)放大现象。

快照读由卷控执行，用在\hl{revert和clone}的场景下。快照读按一定顺序读取多个快照乃至卷。

revert最复杂，是个两阶段过程：既需要保留一部分卷上的数据到snap from或auto snap上(vol -> auto snap)，
然后用读取目标快照，覆盖卷上数据(target snap -> vol)。

几个相关属性
\begin{enumbox}
\item fileinfo.snap version
\item fileinfo.snap from
\item fileinfo.snap rollback
\item chkinfo.snap version
\end{enumbox}

\hrulefill

ROW1是LSV项目开始时的设计方案，Btree实现索性，但是只记录写入卷的私有数据，而没有完整copy快照头。
所以对读操作而言，需要一个回溯过程。

ROW2采用了完整的快照头做索引，采用了类似内存页表的数据结构。这是因为LBA是非常规则的线性地址空间。
内存、inode、ext3的间接块都采用了类似结构。

ROW 2/3的不同：一个chunk逻辑上多个LBA（以页为管理粒度），是否对应一个裸卷的chunk。
分析不同读写模式下性能和空间的影响。读写/顺序、随机可有四种组合。ROW2在随机写的情况下，会导致大量空间浪费。
ROW3则碎片化严重，顺序读性能需要补偿。

ROW2保留chunk内的相对顺序，随机写会浪费大量空间。ROW3以append方式写入，碎片化问题严重。

bitmap cache，有点类似页表的TLB。

bitmap记录LBA到PBA的映射关系。PBA引用到本卷或别的卷的一个物理页（chkid,chkoff, chk内页偏移）
clone时候需要指向外部卷，跨卷访问。

LBA如何映射到PBA？连续的LBA是否映射到连续的PBA上？

bitmap组织成二级索引结构。L1 bitmap私有，L2bitmap共享，发生COW。

\hrulefill

LSV的why、how和what？

LSV适用于什么场景？对标的系统有:
\begin{enumbox}
\item HDFS/HBase
\item LevelDB/RocksDB
\item NOVA文件系统
\item Open Vstorage
\item SSD/FTL（磨损均衡）
\item LSMT
\end{enumbox}

\hrulefill

数据结构和算法
\begin{enumbox}
\item buddy algorithm
\item graph dfs/bfs采用三色标记
\item radix tree/radix sort, bst/quick sort
\item trie tree, prefix用于压缩，postfix用于匹配
\item DPDK的lock free ring
\item DPDK/SPDK生态：
    \begin{enumbox}
    \item oracle RAC/ASM
    \item QEMU virtio/vhost
    \end{enumbox}
\item QoS
\item snapshot
\end{enumbox}

为什么说FP适合于并发编程？
