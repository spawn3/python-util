\section{02}

\subsection{02}

\hl{Reactive Programming与CAP关系密切}。追求的value是即时响应、可维护性和可扩展性。在正常情况下是如此，即便发生故障，或负载变化，也是如此。
如何做到？引入了async、non-blocking消息传递，引入了显式的消息队列。通过MQ，如何实现以上诸特性呢？

系统划分成组件树的成绩结构，明确职责、隔离故障，并把故障委托给独立的故障处理单元。

\hrulefill

Lich的故障处理，应该有一套框架去处理所有故障，包括磁盘、节点，甚至把balance也纳入其中。
通过发送消息来触发相应的处理逻辑。

维护显式的消息队列，放在哪儿？client端，卷控端？如放在卷控端，由谁来调度执行？
卷控实则是个非执行体，只是一个内存结构。

可以由卷控所在的core线程去调度、或采用timer的方式。

MQ的task之间，如果由依赖关系，如io读写请求，需要同步机制。

并发提交的每个请求对应一coroutine，在scheduler层面，形成了一MQ。

\hrulefill

Lich的task，与actor有很多不同，如没有mailbox，不能形成监督树。

\subsection{04}

明体达用，理论是体，在各种系统中的实现是用。整理分布式存储系统的方方面面，不能脱离具体的工程实践去抽象地理解理论。
Lich也是诸多系统之一。

按自然的演进过程，从ACID到CAP。单机事务是思考的起点，CAP以及其改进是设计原则。
加入复制后，情况变得更为复杂。

ACID中，I是隔离性，需要CC机制来保障。A/D是原子性和持久性，需要UNDO、REDO日志来保障。C是业务数据完整性约束。
与CAP中的C有所不同。CAP中的C是原子读写，如线性一致性、顺序一致性、因果一致性、会话一致性。

CAP也是数据库的设计的指导原则。\hl{CAP的最新形式是PACELC，与Reactive Programming很类似}。
分布式存储系统是现代大规模高并发网站架构的一个主要部分，按分形学，有很多共性。

线性一致性对读写操作都要执行全序原子多播。顺序一致性需要对写操作执行全序的原子多播。

一致性可以看作物演通论的生存度，一致性递弱需要相应的补偿协议，从强一致性到最终一致性是一个连续的光谱。
一致性需要高于某一阈值，系统才是健康的。一致性降低必有相应的代偿协议。

为什么要降低一致性？或为什么一致性会降低？

\hl{single master与primary copy}不同，前者针对的是真个数据集，后者针对的是某一数据项。
还有一种peer to peer系统，就是一个数据项的更新也会涉及多个primary。

两者之区分实质上也就是\hl{paxos协议中无主和有主的区分}。我们把RAFT看作paxos协议的一种实现。
通过强化约束，来简化实现的复杂度。

\hrulefill

DB引擎，意索引。先考虑CC(隔离性，事务调度器)，次考虑提交协议，最后考虑基于日志的故障恢复(A/D)。
扩展到分布式数据库系统，需进一步考虑\hl{CC和提交协议如何去满足ACID属性}。

整理数据库相关知识，包括单机、分布式数据库，从数据模型上，包括SQL/NoSQL/NewSQL。
主要从几个维度进行：
\begin{enumbox}
\item 数据分布
\item 复制
\item index
\item transaction
\item 读/写
\end{enumbox}

\hrulefill

\hl{CAS/ABA，时间戳，向量时钟}。

MVCC里对每行记录增加两个隐藏version。

除了IO之外，还有并发的其他任务，如replica cleanup。

有多种CC方法，主流的是lock，\hl{TO、MVCC都是基于版本号/timestamp}。

\hrulefill

把以上理论进一步落实到Lich和Ceph的设计中。
\begin{enumbox}
\item 更新chunk时递增info version？
\item 在snapshot也实用了version。
\end{enumbox}

\subsection{05}

同心圆模型，最核心的是设计心法，从内往外以此是硬件，软件(OS、FS、DB、PL/Compiler)，网络，分布式系统。
设计和算法是两大重点和难点，可放入最内圈。

可靠性、可用性，容错、resilient，可伸缩性、elastic，可扩展性等概念有着细微的差别。

性能\&故障诊断分析方法：
\begin{enumbox}
\item Pipeline
\item Queuing
\item ***
\item PAT
\end{enumbox}

数据存在的几种形态：
\begin{enumbox}
\item Storage
\item Cache
\item MQ (stream)
\end{enumbox}

Storage和Cache具有极大的相似性。

至于async更多是一种架构思想。Reactive Programming强调了MQ的重要性，\hl{message-driven是实现RRE的良方}。

动静分离的静，最适合应用多级cache，御敌于国门之外，最终到达源站。
无状态的业务逻辑层与有状态的数据层，如何进行伸缩是有很大不同。

\subsection{07}

数据层面：复制(EC、dedup、compress)、snapshot、cache等等。

卷的snapshot，cc中快照隔离，日志的检查点技术

\hrulefill

adesk架构演进之路:
\begin{enumbox}
\item 单库无拆分->动静分离->master/slave的replication架构。
\item 加入ElasticSearch，支持全文搜索功能。
\item 采集行为日志，进行数据分析。google analysis，umeng等。
\item 对接广告平台进行变现
\item \hl{上云}：不再自己运维。采用了阿里云和七牛的云平台。
\end{enumbox}

性能：索引，cache(CDN etc)、负载均衡

可伸缩性：动静分离、增加磁盘。

推送

监控：第三方监控、nagios、zabbix

增长黑客

\subsection{17}

\hrulefill

进入\hl{刷题模式}，有的放矢、定量有恒地刻意练习，以刻意练习的理念概括之。

paxos的二阶段可用温故知新概括之。

membership change比较复杂，joint联合共治，多数派原则？

由CAP定量出发，来理解分布式系统的复杂性。CAP/PACELC过渡到Reactive Manifesto。

分布式一致性至关重要，相关协议有:
\begin{enumbox}
\item 2PC/3PC
\item paxos
\item pacificA
\item raft
\end{enumbox}

pipeline、SEDA、lambda，软件管道三原则。

可以数据库的事务管理来类比：\hl{cc和故障恢复}。cc有2pl和基于时间戳的多版本机制。
故障恢复基于log/journal，支持redo和undo操作。

ACID的A/D关乎故障恢复，I关乎并发控制。事务的隔离级别，最少要read committed。因为可以undo，不加控制的话，会出现read uncommitted。

提交不变性：

最大提交原则：提交的一定满足A/D，未提交的如何处理？

所谓顺序一致性就是可以交叉，但要看到共同的交叉顺序。\hl{意味着并不需要UNDO操作？只要原子写入}？
\hl{顺序一致性}，lich支持顺序一致性，可能读到uncommitted数据，由上层应用来保障完整性。

分布式计算：在分布式环境中，强一致性因全局时钟限制很难实现。程序员开始寻找条件不那么严苛的模型，
lamport利用逻辑时钟代替全局时钟提出顺序一致性，其定义如下：
\begin{enumbox}
\item 所有执行结果相同，就像处理器的所有操作是在一定的顺序下执行的一样。
\item 在本地时钟下，本地操作顺序为本地程序的执行顺序。
\end{enumbox}

不同处理器之间的操作出现重叠是可能的，但顺序一致性要求所有处理器看得的重叠相同。

顺序一致性需要客户端的补偿措施。如并发访问文件的一个区域时，由client自己进行同步。这对块来说是合理的。
对不带journal的文件系统是否有问题？

顺序一致性和read uncommitted的事务隔离级别是两个不同概念，宜分开说。
先考虑顺序一致性(分布式系统的顺序一致性如何实现)，即便实现了顺序一致性，还需考虑整体故障情况下会发生read uncommitted的问题。
\begin{enumbox}
\item 分布式系统如何实现顺序一致性
\item 如何处理整体故障情况下的read uncommitted问题？
\end{enumbox}

事件因果序由逻辑时钟、向量时钟、矩阵时钟定义。分为两部分：本地所见的全局时间、本地逻辑时钟。
在逻辑时钟里，两部分压缩为了一个变量，所以逻辑时钟并非强一致性的，即由逻辑时钟推导不出事件的前后顺序。

分布式计算：顺序一致性不如线性那么严格，相比线性更容易实现。虽然存在全局顺序，但是本地处理器执行操作顺序未必一定要严格遵守全局时间顺序，
\hl{整个过程只需要对写操作使用全序广播机制}，并不需要对读操作使用全序广播。

lich采用了单控和clock，无journal，clock没有持久化。通过lease机制来保障卷控的唯一性。
vip机制用来寻址iscsi target。(iscsi targt和卷控通常在一个节点上)

保序：iscsi initiator生成一个序号，或者根据session？这里有什么问题？
提交到一个卷控，然后该\hl{卷控failover}，然后呢？

换个角度来看，lich卷提供了类似裸盘的东东，操作系统等上层应用对裸盘有什么要求和保障，
需要做什么才能保障业务数据完整性？ lich的client端与lich卷之间的关系也是如此。
(RMI：关系映射反演，是化归原则的进一步抽象？)

io大小对一致性的影响？就地写，而不是多版本下的shadow page、或COW写。会带来什么样的问题？

多控会把顺序一致性问题变得复杂。用单控和clock可以实现顺序一致性（logical clock）。所有元数据和数据都有卷控集中处理。
卷控的存在，把问题简单化了，但会引入新问题，如读写局部性/本地化。Ceph rbd(rich client)如何处理该问题？
\hl{什么场景下多个rbd client会访问一个卷的相同区间}？

如果把这个问题推到多控场景呢？ 引入KV之后，元数据/数据缓存面临一样的问题，共同构成共享状态。
多控下如何考虑Multipath。或者说配置multipath会导致多控问题。multipath下的多各卷控什么关系？是主备还是双活？
仅仅是容错，还是可以聚合带宽？ 用lease、cas机制能实现多控吗？

\hl{多控下如何实现顺序一致性}？ vector clock、paxos？

\hl{应用场景}：orache rac、asm是什么样的架构？多个qemu实例会不会有共享卷？

CAP、质量三角形都体现了\hl{递弱代偿原理}。一致性的下降需要补偿措施。
ROW2/3提高了write操作的性能，可增加了元数据，来补偿Read操作。
写和读也是平衡关系，特别是\hl{随机写、顺序读}两者之间。

引申到事务管理，顺序一致性更多是前端并发控制的问题，而read uncommitted则是事务本身面临的问题，
需要与client结合在一起作为一个整体来考虑。
也可以\hl{用RMI原则来分析}：原领域（lich）- 映射到事务管理（cc和recovery）。

没必要说lich是什么顺序一致性，从复制状态机的角度去描述更好。
用raft协议去说明写是如何提交的，又如何读。

\hrulefill

快照：COW、ROW 1/2/3、LSV。

快照不变式，由快照读定义。

vmware如何实现快照？每个卷/快照对一个一个或多个文件，粒度更细，更易均衡管理。

open vstorage打快照非常容易，how to read，问题依然是读操作的性能，如引入快照头的开销如何？

如果卷和快照按1:1映射关系，还有必要堆叠在裸卷之上吗？直接实现一种与COW同级的ROW1/2的卷格式如何？
这样管理粒度更小，更易于灵活组织。采用非堆叠方式，只需要一层元数据即可。

先理解当前实现的COW快照树。主要问题有几个：
\begin{enumbox}
\item 卷和快照如何组织，与底层裸卷是什么关系？1:1，还是n：1?
\item 维护快照树
\item 快照头有无完整索引？索引采用什么结构？
\item 如果有索引，索引的管理粒度是多大？page？
\item 如何保障\hl{读的不变性}？ how to read
\item 每种操作的性能和复杂度
\item 支持的快照数量
\end{enumbox}

在混合存储里，两层元数据最好都落到高速层上。由底层空间分配器进行管理。

ROW3相对于ROW2的管理粒度更小（从chunk到page），ROW2的chunk内偏移是可以通过计算获得的。
这也显示了一种管理映射关系的设计模式，{是通过元数据还是通过计算}？
sheepdog、ceph都是基于DHT进行计算的。

分析写和读过程。新create的快照无数据，后续也只是存储私有数据。
所以没有完整索引，会有读(快照读，而不是卷读)放大现象。

快照读由卷控执行，用在\hl{revert和clone}的场景下。快照读按一定顺序读取多个快照乃至卷。

revert最复杂，是个两阶段过程：既需要保留一部分卷上的数据到snap from或auto snap上(vol -> auto snap)，
然后用读取目标快照，覆盖卷上数据(target snap -> vol)。

几个相关属性
\begin{enumbox}
\item fileinfo.snap version
\item fileinfo.snap from
\item fileinfo.snap rollback
\item chkinfo.snap version
\end{enumbox}

\hrulefill

ROW1是LSV项目开始时的设计方案，Btree实现索性，但是只记录写入卷的私有数据，而没有完整copy快照头。
所以对读操作而言，需要一个回溯过程。

ROW2采用了完整的快照头做索引，采用了类似内存页表的数据结构。这是因为LBA是非常规则的线性地址空间。
内存、inode、ext3的间接块都采用了类似结构。

ROW 2/3的不同：一个chunk逻辑上多个LBA（以页为管理粒度），是否对应一个裸卷的chunk。
分析不同读写模式下性能和空间的影响。读写/顺序、随机可有四种组合。ROW2在随机写的情况下，会导致大量空间浪费。
ROW3则碎片化严重，顺序读性能需要补偿。

ROW2保留chunk内的相对顺序，随机写会浪费大量空间。ROW3以append方式写入，碎片化问题严重。

bitmap cache，有点类似页表的TLB。

bitmap记录LBA到PBA的映射关系。PBA引用到本卷或别的卷的一个物理页（chkid,chkoff, chk内页偏移）
clone时候需要指向外部卷，跨卷访问。

LBA如何映射到PBA？连续的LBA是否映射到连续的PBA上？

bitmap组织成二级索引结构。L1 bitmap私有，L2bitmap共享，发生COW。

\hrulefill

LSV的why、how和what？

LSV适用于什么场景？对标的系统有:
\begin{enumbox}
\item HDFS/HBase
\item LevelDB/RocksDB
\item NOVA文件系统
\item Open Vstorage
\item SSD/FTL（磨损均衡）
\item LSMT
\end{enumbox}

\hrulefill

数据结构和算法
\begin{enumbox}
\item buddy algorithm
\item graph dfs/bfs采用三色标记
\item radix tree/radix sort, bst/quick sort
\item trie tree, prefix用于压缩，postfix用于匹配
\item DPDK的lock free ring
\item DPDK/SPDK生态：
    \begin{enumbox}
    \item oracle RAC/ASM
    \item QEMU virtio/vhost
    \end{enumbox}
\item QoS
\item snapshot
\end{enumbox}

为什么说FP适合于并发编程？

\subsection{18}

ceph的pg也是一种分区机制。从sharding/replication的组合看分布式系统的选择。

小米开源了pegasus键值存储系统，table/partition/replica，table隔离。

lich的最基础单元是卷，卷没有做分区处理。iscsi target和卷控绑定到同一个core上。非iscsi的场景呢？

lich的pool采用name作为id，无法rename，无法保障唯一性。
如果删除/再创建，无法判定是否是同一个pool（ABA问题）。

公有云很难采用单一卷控方式，如果在client端直接访问chunk层（此时client即相当于卷控）。如果有多个client并发访问呢？
就多卷控并发访问。如果做并发控制？如果client端有cache，如何做到cache一致性？(CAS/lease)。

聚合边界：\hl{kv、列存、mongodb，都采用了聚合的概念，聚合内部保障原子性}，如kv的一条记录、列存的一行、mongodb的一个document。
聚合之间难以处理原子性问题。为了计算某些统计量，适合用MR计算框架。

cassadra采用了hbase和dynamo的混合。底层存储用列存、用DHT实现分区扩展性。

SQL-on-Hadoop，这种\hl{NewSQl设计范式}在进一步发展，如Spanner、OceanBase、TiDB等。大多基于列存？

\hl{全闪新架构}：C/M/D，client，Meta and data。Meta采用分布式键值存储。
带来的好处是： 剥离vol元数据道M，无table1、table2了，且client可直连data，无需target/卷控转发。

cinder是控制平面的事，无关IO。

\hrulefill

RMI是一种高度抽象的原则，有利于知识的迁移和类比。同样的问题和模式发生在不同的领域，
如果能这样\hl{触类旁通}，极大地提高了学习效率。

盘阵-分布式-数据中心，双活或多活方案，双控有三种模式：\hl{主备、双活（ALUA、SLUA）}。
双控都甩出同一个物理LUN，host端通过多路径软件连到两个LUN上，盘符删除、路径选优。

\hrulefill

\hl{赋值器、分配器、回收器}。弱分代假说，优先回收年轻代的对象。通过check queue和heap来巡回检测。
heap满则溢出进行回收。

什么条件沉入老年代？老年代在full gc里进行处理。full gc的处理策略是什么？

order是特别重要的一个概念，一致性即是定义在时钟的基础之上。因果序分为两个部分：进程内以及多个进程之间的消息。

info version和sqlite里的meta version什么关系？在删除一对象时，meta version有什么用？ 后续已更新，则不允许删除。类似cas机制？

多版本？

\hrulefill

HERE

遗留问题
\begin{enumbox}
\item \hl{RAFT/paxos}，重中之重
\item etcd在lich的用法
\item ***
\item Lich一致性、恢复和平衡
\item QoS token bucket and leaky bucket
\item ***
\item COW snap tree
\item ROW1/2/3
\item LSV
\item ***
\item lich架构
\item RDMA/DPDK/SPDK
\end{enumbox}

数据结构和算法
\begin{enumbox}
\item hash
\item skiplist
\item heap
\item buddy algorithm
\item lock free ring
\item 大量数据的情况
\end{enumbox}

其它
\begin{enumbox}
\item 多控和多路径
\item Orache ASM
\item Orache RAC
\item Qemu virtio/vhost
\item GC
\item 如何管理文件系统元数据
\end{enumbox}

\subsection{19}

用bst统一去理解二分查找、merge sort and quick sort。merge sort可以看作bst的postorder transval，
quick sort是preorder scan，pivot就是root。

从CAP角度去理解分布式的why，从逻辑时钟的角度去理解分布式的how，序的概念异常重要！
包括分布式共识、事务的串行化执行等。

\hrulefill

COW不能做全量快照头，因为引用的是卷，而卷的数据是变化的。
ROW则可以，引用的是snapshot，snapshot一旦创建就不变了。
snap tree分析COW各项操作的好方法。

ROW必须管理到页，为什么？否则，写入的时候也会发生COW。

LSV架构更多借鉴了open vstorage。
GC check效率太低，如何利用有用信息改进？bitmap在update时的信息是有一定启发作用的。

引入auto snap并不是必须的，如果不引入，则存入snap from，无法有效回收，导致空间浪费。

\hrulefill

反思lich架构设计
\begin{enumbox}
\item meta采用vfs的方式，过于复杂，解耦为独立的kv服务？
\item 卷和快照对应不同的卷控，可能映射到不同的core上
\item multipath
\item 引入ceph pg优化lich元数据？
\end{enumbox}

\hrulefill

\hl{chkinfo.snap version}就是创建快照时卷的snap version，
卷的snap version一直是最大的，且随着创建快照的过程在单调递增。

chunk.snap version在各种操作中是如何变化的？
该属性只在vol上有用，snapshot上不维护，发生cow之后，snapshot该属性初始化为0。
\begin{enumbox}
\item cow后，设为fileinfo.snap version
\item revert后，设为fileinfo.snap rollback
\end{enumbox}

revert后，vol上chunk都被置为snap rollback，可用于判定rollback是否完成。
后续发生cow时，会检测该chunk已经在snapshot上存在，则跳过，不发生实际的cow。

\hl{chkinfo.info version}用于记录chkinfo信息的变更，该信息用于unlink时检测是否合规。
如传入的值小于当前值，说明已有更新，则拒绝。

数据库的meta version与该值什么关系？要解决什么问题？即是为什么要用？

副本上用clock来跟踪写入。

\hrulefill

重删定长压缩比很低，变长实现难度很大，需要在pool级做。

怎么在lsv理念下解决压缩、重删问题？

EC、重删、压缩能否offload到设备上？
