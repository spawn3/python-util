\chapter{模型}

\begin{tikzpicture}[show background grid]
    \begin{class}{Disk}{6, 0}
    \end{class}
    \begin{class}{Storage Pool}{6, 2}
    \end{class}
    \begin{class}{Volume}{6, 4}
    \end{class}
    \begin{class}{Host}{6, 6}
    \end{class}
    \begin{class}{Cluster}{0, 2}
    \end{class}
    \begin{class}{Snapshot}{0, 4}
    \end{class}

    \composition{Cluster}{pools}{1..*}{Storage Pool}
    \composition{Storage Pool}{disks}{1..*}{Disk}
    \composition{Storage Pool}{volumes}{1..*}{Volume}
    \composition{Volume}{mapping}{*..*}{Host}
    \composition{Volume}{snapshots}{1..*}{Snapshot}
\end{tikzpicture}

\section{Cluster}

整体

\section{Protection Domain}

把物理节点划分为不同的保护域，一个卷的所有数据只出现在一个保护域内。卷可以跨保护域进行复制和迁移。

默认一个，包括所有节点。

% 保护域是物理节点的划分，存储池是存储介质的划分。每块盘只能出现在一个存储池里。

\section{Pool}

逻辑容器

\section{Storage Pool}

与存储池有什么同和异？存储池可以看做关联了磁盘的pool，可以看做pool的子类。

属性：
\begin{enumbox}
    \item 磁盘列表
    \item 定义精简池
    \item 存储池上可以指定卷的副本数
    \item \hl{有足够的故障域，且不同故障域配置一致的资源量}
\end{enumbox}

操作：
\begin{enumbox}
    \item 创建
    \item 删除
    \item 扩展（添加磁盘到\hl{已存在的存储池}，该映射关系持久化到本地，同步到admin节点）
    \item 缩容（从存储池中移除磁盘，引发数据重建过程）
    \item \hl{自动或手动按磁盘速率进行存储池分级划分}
    \item 不同存储池之间，卷的复制
    \item 不同存储池之间，卷的迁移，可在线或离线
    \item 存储池级别的统计信息
\end{enumbox}

% 存储池是disk的集合，与节点无关。但disk所在的节点构成存储池的节点列表，不同存储池的节点可能覆盖。

存储池下，可以创建volume。没有关联磁盘的存储池，不能创建卷。

\hl{chunkid到磁盘物理位置有两级映射：chunk的副本节点列表，节点内chunkid到物理地址的映射}。

在为卷分配chunk的时候，需要确定各个副本的物理存储位置。当前实现是返回不同副本的节点列表。
如果指定了存储池，就需要在存储池所在的节点范围内进行分配。同时要满足故障域和数据均衡规则。

\begin{tcolorbox}
移动采集中存储池要求，相比于目前的逻辑pool，更多是一种设计上的退步。
存储虚拟化的目标，是物理位置无关。我们可以基于逻辑容器，实现基于策略的管理。
所以，\hl{从实现层面，要保留当前pool的功能，按照系统配置确定pool的类型}。
\end{tcolorbox}

% 存储池内，要满足故障域规则(\ref{rule:faultset})

\section{Fault Set}

故障域有粒度之分，如磁盘，节点，机架，机柜，数据中心。

存储池内，要满足故障域规则：一个chunk的不同副本，分布在不同的故障域内。\label{rule:faultset}

在初次分配，再平衡和恢复等过程中，都需遵循这些规则。

\section{Volume}

属性：

操作：
\begin{compactenum}
    \item rename
    \item resize \info{在线扩容}
    \item mv
    \item copy \change{全量拷贝/增量拷贝} \change{跨存储池拷贝} % change不能出现在box里
\end{compactenum}

\section{Snapshot}

snapshot隶属于卷，无卷则无快照，快照组织成快照树，其中有且只有一个快照是可写快照，即卷的写入点。

\section{Mapping}

数据隔离/ACL，数据保护

卷对主机的可见性。一个卷只有映射给了某主机，才可以被该主机访问。

采用白名单机制，但是，cinder需要无验证地访问一个pool。

在建立mapping时，需要host信息。如果有host列表，则可让管理系统去选择。

iscsi采用chap认证。

每个pool上，需要一个属性，表明是开放的，还是封闭的访问模式。如果是封闭的，检查白名单进行认证。


\section{Consistency Group}

一致性卷组

\begin{shadequote}
Consistency Groups could be useful for Data Protection (snapshots, backups) and
Remote Replication (Mirroring).

The Mirroring support will allow to setup mirroring of multiple volumes in the
same consistency group (i.e. attaching multiple RBD images to the same journal
to ensure consistent replay).

There is already an interest to implement this functionality as a part Mirroring feature:
http://tracker.ceph.com/issues/13295

The snapshot support will allow snapshots of multiple volumes in the same
consistency group to be taken at the same point-in-time to ensure data
consistency.
\end{shadequote}
