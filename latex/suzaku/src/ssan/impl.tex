\chapter{实现}

\section{ETCD}

\subsection{Pool}

\mygraphics{../imgs/etcd/etcd-pool.png}

\subsection{Metadata}

\mygraphics{../imgs/etcd/etcd-metadata.png}

\subsection{Coreid and Diskid}

\mygraphics{../imgs/etcd/etcd-instance.png}

\subsection{Network}

\mygraphics{../imgs/etcd/etcd-network.png}

\section{Schedule}

\mygraphics{../imgs/core-t.png}

不能支持嵌套task，用pre yield变量来控制。

单个节点多个进程core的分配策略

协程有种种用法，如单次加载的wait-post模式。

协程的stack是在堆上，注意其大小，防止出现溢出现象。

\begin{myeasylist}{itemize}
& cpu上运行的是哪个进程？
& polling core不能有slow操作。\hl{如何定位slow操作？}
\end{myeasylist}

命令序列
\begin{myeasylist}{itemize}
& top - 1
& strace
\end{myeasylist}

\section{variable}

每个core线程的重要数据结构的指针数组。根据TLS的\_thread\_id\_进行CRUD。

\hl{为什么不存在core\_t上}？只需要获取线程对应的core\_t即可。

\section{内存}

\subsection{需求分析}

需求
\begin{myeasylist}{itemize}
& 接口简单
& 两层allocator
& 多线程(TLS)
& 不同core可以配置不同数量的内存，甚至动态化
& 内存池多实例
\end{myeasylist}

抽象出公共接口，可以在\hl{hugepage和malloc两种模式}之间进行切换，以方便测试。
\begin{myeasylist}{itemize}
& 分配2M的页对齐的内存
& 分配小块内存
\end{myeasylist}

两层内存分配器，第一层分配2M大小的文件，第二个分配任意大小的小内存。
tiny allocator是不对齐的。buffer所用allocator按页对齐。

区分\hl{有状态和无状态的代码}。在无状态代码之上构建有状态的代码。

malloc模式下，无对应phyaddr，故不能直接用于NVMe等场景，需要v2p过程。

\subsection{Hugepage}

从初始化顺序上来看，分为hugepage初始化之前和之后。初始化后才能用hugepage相关接口。
hugepage供两种线程使用，一般线程和core线程。
所以分为多种区域，一个公共部分和每个polling core对应的部分。

在初始化hugepage之后，所有内存分配尽量走hugepage的接口。

提供什么接口，三种生命周期范围、持久性：
\begin{myeasylist}{itemize}
    & 常驻
    & session
    & IO
\end{myeasylist}

使用场景
\begin{myeasylist}{itemize}
    & \hl{sche\_task}
    & core private memory
    & buffer\_t
    && libnvme
    & little object
    && ring
    & RDMA
    & NVMf
\end{myeasylist}

怎么抽象一般内存和hugepage-based内存？

每个对象内嵌一个buddy对象管理hugepage的分配、释放。
另外，从core的MR里，利用buddy算法分配连续内存，用于ring等小对象。

用面向对象的方式处理，每个core对应一个MR对象。public的也是如此。

采用buddy算法管理连续内存分配

动态化

\mygraphics{../imgs/hugepage.png}

抽象出head，core和public重用代码。第一选择head，第二执行head的操作。

如何使用超过2M的内存？如htab的array。

禁止在一个core内malloc，由另外一个core进行free。

frctl、mdctl、bactl如果用同一的逻辑申请hugepage，会造成浪费？

\subsection{buffer}

\mygraphics{../imgs/buffer-t.png}

每个内存区域的\hl{第一个hugepage用来保存该区域的元数据信息}，可供分配的是后面的hugepages。
在元数据信息中加上buddy，可用来支持buddy算法。

buffer的每个seg都包含有虚拟地址和物理地址。

多个seg的buffer如何管理，特别是进行IO的时候，底层结构需要连续的内存区域，甚至是\hl{物理连续的内存区域}。

\subsection{Tiny Memory Pool}

\mygraphics{../imgs/memory-pool.png}

tiny mem公共内存区域分配的内存，由core线程去free，导致的问题。

直接从hugepage申请内存，从hugepage申请一个hugepage，用于小对象。pool管理多个size的小对象队列。
根据要malloc的size，定位到队列。

free时按指针查找属于哪个hugepage。每个hugepge对应起始地址和结束地址以及所在队列的标识。
这样可以保留malloc和free的语法和语义。

hugepage层只需要提供分配单个hugepage的接口，一个队列可以由一个或多个hugepage构成。

% 或者，memory pool按4k进行组织，同样采用buddy算法。在其上实现ring等。

\hl{每layer都要动态化，包括增和减}。

\subsection{NVMe}

NVMe为什么需要物理地址？

direct io需要512对齐。

\subsection{IO}

\mygraphics{../imgs/io-align.png}

首尾页对齐 

buffer\_t包含一个seg时，方便处理。如果有多个seg，是否需要分配连续的大块内存。

\hl{SPDK的大IO问题}：NVMe需要物理内存，并且一次io物理内存是连续的。
malloc的内存，不容易找到物理内存。
2M的hugepage虽然能获取虚拟地址连续的4M地址空间，但底层物理内存未必连续。
用1G的hugepage更容易管理。

\section{Disk}

% \mygraphics{../imgs/etcd-suzaku-instance.png}

通过独立线程scan到各个disk，放入slot中。并注册到etcd。

\mygraphics{../imgs/bactl/diskid-slot.png}

全局唯一的diskid放在etcd上。根据diskid查找到slot idx。根据slot idx索引到disk。
即可执行IO操作。

\mygraphics{../imgs/bactl/disk-layout.png}

IO的offset是chunk内偏移，需要转化为文件内偏移。

磁盘有2M的头部，用于保存相关元数据。

\section{Network}

corerpc采用TCP或RDMA。

从TCP到RDMA，两者可以共存。编程模型上有同有异。问题
\begin{myeasylist}{itemize}
& 如何进行连接管理
& IB用什么建立连接，也是IP地址和port
& core与core之间两两建立连接
& 如何整合进epoll体系
& 两个core之间单连接还是双连接
& 管理网和存储网
& 如何同时支持TCP和RDMA
& 如何支持多个设备
& 支持两种模式：polling和event
& rpc/net/sock的分层逻辑
& NVMf采用了自己的RDMA方式
\end{myeasylist}

\subsection{TCP}

\subsection{RDMA}

\mygraphics{../imgs/ibv-devices.png}

% \http://donggj:taiji123Jing@gitlab.taocloud.com/qwq/suzaku.git

\begin{myeasylist}{itemize}
& ibstat
& /usr/sbin/cma\_roce\_mode
& nvme discover -t rdma -a 192.168.201.31 -s 10060
& nvme connect -t rdma -a 192.168.201.31 -s 10060 -n <subnqn>
& nvme disconnect -n <subnqn>
\end{myeasylist}

主线程管理连接，加入core线程，用于rpc和polling。每个core对每个devic维护一个CQ。

\mygraphics{../imgs/rdma-object.png}

规则
\begin{myeasylist}{itemize}
& 在connect request阶段分配连接相关资源，包括qp、memory、post recv等，并注册到cm\_id上
& receives must be posted before sends
& comp\_channel
& 双线程结构：event and comp channel
& 外部线程监听，连接加入corerpc
\end{myeasylist}

把event channel加入epoll即可。在事件处理程序中，通过rdma\_get\_cm\_event来整合进epoll体系。
进一步通过event type调度到不同的event handler里。

\hrulefill

每个core创建event channel，listen不同端口。运行polling过程。数据连接提交给polling core处理。

在刚建立连接时，分配连接相关资源，如qp，mem等，由\hl{cm\_id->context}跟踪特定连接信息。
每个连接$1024*512$内存。

从\hl{commit和poll两个方向}去理解RDMA的工作原理。commit不需独立线程。

以写流程为例。写从client直达bactl。

\mygraphics{../imgs/rdma-mapping.png}

\hrulefill

ibverbs连接问题：libsuzaku动态或静态连接libibverbs，都发生问题，甚至连最普通的函数返回结果都是错的，如ibv\_get\_device\_list。
把libsuzaku做出静态库，就可以了。为什么呢？

排查问题的过程，一波三折。开始怀疑是代码问题，后来定位到libibverbs的调用边界。怎么会怀疑到\hl{libsuzaku是静态库或是动态库}呢？

把suzaka\_ls单独拿出来，调用最简单的API，都是成问题的。偶然性地想到链接到静态库的可能性。

\subsection{RPC}

\section{Partition}

\mygraphics{../imgs/partition/partition-update.png}
