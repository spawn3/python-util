\chapter{系统架构}

\section{硬件架构}

\subsection{通用服务器}

系统运行在通用服务器之上，多台服务器组成分布式存储系统，按硬件配置提供相应的服务。

\subsection{磁盘}

支持SATA SSD、NVMe SSD。

\subsection{网络}

推荐RDMA网络，包括IB、支持RoCE v2的以太网等。

\section{软件架构}

% \mygraphics{../imgs/arch/system-arch.png}

\mygraphics{../imgs/suzaku/suzaku-function.png}

\subsection{块级虚拟化}

提供标准的iSCSI、iSER、NVMf target，可用于各种场景下的块存储服务。

\subsubsection{iSCSI/iSER}

\mygraphics{../imgs/target/iscsi-structure.png}

\subsubsection{NVMe over Fabrics}

\mygraphics{../imgs/target/nvmf.png}

\mygraphics{../imgs/target/nvmf-target.png}

NVMf的贡献在于提供除PCIe外访问NVM的另一个途径-Fabrics，并且将fabrics链路在latency上增加的overhead维持在10us以内。

提供fabrics途径后，可以在其他节点直接访问NVMe设备，
那么最基本的应用就是替代传统的iSCSI，在闪存系统中导出NVMe。

现在支持NVMf运行在RDMA transport(IB or RoCE v2)之上。

分为host端和target端，host端采用工具nvme-cli，target端采用SPDK的NVMf target，
完成协议解析后，对接到后端suzaku存储系统。

一个卷可以通过任一NVMf target进行挂载，赋予全局唯一标识NQN。

% redirect

\subsection{分布式架构}

\mygraphics{../imgs/arch/modules.png}

\mygraphics{../imgs/arch/message-flow.png}

分布式架构具有良好的可扩展性，可以方便地进行垂直或水平扩展。

\subsubsection{ETCD集群}

ETCD集群用于选举master节点，还用于保存集群配置信息、部分元数据信息，参考元数据服务一节。

这部分数据数据量不大，且访问频率低，采用etcd存放易于管理，可以带来很大的灵活性。

\subsubsection{进程和服务}

suzaku采用对称式分布式架构，集群中所有节点具有相同的进程结构和目录结构，提供相同的服务。

每个节点上启动两个suzaku进程：ioctl和mdctl。ioctl进程提供tgtctl、frctl、rangectl、bactl等服务。
各服务按指定配比关系映射到CPU核上，每类cpu专注于特定的任务，系统充分考虑了NUMA多核架构访存的局部性特征。

通过tgtctl导出target服务。tgtctl转发io请求到本节点内同一NUMA节点上的frctl，frctl与rangectl、bactl协作，共同完成io处理工作。

rangectl是子卷控制器，负责协调卷的一部分数据的io、修复、平衡等任务。
rangectl向mdctl查询某一slice的位置和状态信息。

bactl负载管理本地多块磁盘，提供数据的写入和读取服务。

mdctl是一KV数据库，提供元数据服务。其中一个mdctl进程充当master角色。

\subsubsection{集群管理}

suzaku采用ETCD选举master节点，选出的master节点负责分配磁盘空间，及其集群级的协调任务。

故障情况下可重新选举、自动切换，具有高可用性。master节点负载不大，不会形成系统的瓶颈点。

\subsubsection{副本一致性}

副本一致性采用RAFT一致性协议。

为了提高单卷的扩展性，每个卷分成多个子卷，每个子卷对应有子卷控制器，可以看作RAFT协议里的leader。

由frctl处理io的读写过程。

写入过程：并发写入各副本，全部成功后返回成功。

读取过程：按负载均衡策略，选取一可用副本，读取成功后返回成功。

\subsection{元数据服务}

\mygraphics{../imgs/arch/dir-metadata.png}

\mygraphics{../imgs/arch/vol-metadata.png}

系统内置元数据服务，元数据分为几个部分：
\begin{myeasylist}{itemize}
    & 集群配置信息
    & 目录结构
        && 存储池 
        && 存储池里的卷
    & 卷的属性和扩展属性
    & 卷的引导信息
    & 卷数据块的副本位置
\end{myeasylist}

一部分存在ETCD上，一部分存在bactl上（包括卷的两级元数据volume slice、range slice）。

不同于基于分布式哈希表的存储系统，在内置元数据的协助下，可以受控地进行数据恢复、平衡等后台任务，
使系统表现更稳定，以最小化对前端业务的影响

一个存储池里卷的数量不受限，可以灵活地根据业务需求创建卷。

每个卷的元数据均匀分布在后端磁盘上，元数据用于定位卷的数据块。支持的最大单卷容量可达4PB。

卷空间划分为固定4M大小的数据块(称为slice)。一个卷的元数据和数据可以看作一个4M slice的三层树。
第一层volume slice指向第二层range slice，第二层range slice指向第三层raw slice。
第一、第二层是卷的元数据，第三层的raw slice构成卷的数据空间。

卷进一步化分为子卷(range)，每个子卷管理128GB的卷空间。
子卷控制器(rangectl)均匀分布在不同的节点上，从而实现卷性能和容量的scale out。

一个slice对应的元数据会缓存在其所属owner rangectl上，并通过mdctl持久化到bactl所管理的盘上。

\subsection{负载均衡}

系统的均衡性体现在许多层面，如数据均衡、负载均衡。

数据均衡有两个过程保证：首次分配过程和再平衡过程。

卷可以通过任一tgt控制器导出，卷进一步划分为子卷，每个子卷负责管理该卷的一部分数据，
子卷通过hash过程均匀分布在不同存储节点的多个子卷控制器上。
这样，数据和负载都能平衡地分布在存储池内的所有节点和磁盘上。

% \subsection{控制路径和数据路径分离}

% \subsection{丰富的存储特性}

\subsection{极致性能}

SSD盘的优势在于随机IO性能好，时延低，劣势在于需要进行磨损均衡，以提升其平均使用寿命。
系统采取了适当措施，充分利用相关硬件特性，以实现更好的可靠性和性能。

在全闪场景下，需要重新考虑传统IO路径的效率问题，操作系统服务、锁争用、硬中断和IO编程模型等都严重制约了SSD的性能。

通过合理地使用cpu核、自定义内存分配器、kernel bypass等技术能充分地发挥全闪的性能优势。

% 系统的不同服务映射到专属线程上，这些线程绑定到独立的cpu核上，有定制的内存分配器负责内存管理，
% 可以避免访问共享内存的锁定机制，极大地提高了性能。

\subsubsection{并行流水线架构}

\mygraphics{../imgs/io-path.png}

一个卷的IO可以调用多个核进行处理。

首先，卷可以通过任一tgt控制器导出，tgt控制器把相关请求分发到多个前端控制器上执行(frctl)。
然后，每个frctl进一步下发请求到子卷控制器和后端控制器，从而实现了专核专用的并行流水线架构。

% tgtctl、frctl、rangectl、bactl、mdctl等服务可以指定对应的cpu核及其数量。

把一个io分解为协议解析、io处理、元数据访问、落盘等阶段，
每个阶段分配独立的cpu核，达到专核专用、低延时的目的。

系统实现充分考虑了NUMA多核架构的访存局部性特性。

根据每节点任务处理的负载，可以设定不同服务的cpu核数配比关系，实现负载均衡，以最大化cpu利用率。

\subsubsection{自定义内存分配器}

操作系统提供的内存分配器，没有考虑cpu亲和性、碎片化严重等问题，会带来极大的系统开销。

采用Hugepage虚拟内存管理机制，一次性从操作系统申请出所需内存，降低页表查询开销。

对每个服务CPU预留内存，可以无锁访问，预留内存不足时，再从公共存储池申请。

同时，通过维护不同尺寸的多个内存资源池，用于小块内存的使用场景。

% \subsubsection{高效的编程模型}
% 现代编程语言如Erlang、Go都采用了基于coroutine的并发编程模型。

\subsubsection{kernel bypass}

采用kernel bypass技术，达到极致性能。可通过用户态驱动访问NVMe SSD，通过RDMA技术释放网络性能，
从而使高吞吐、低延时系统成为可能。

\subsubsection{自平衡}

体系结构的平衡性是影响性能的关键因素。

在扩容或缩容后，数据在磁盘上的分布可能处在不平衡状态，导致磁盘参与度有高有低，系统需要能够重新回到平衡状态。

独立的后台任务调度器按预定策略执行数据再平衡任务，保证每个卷的数据均匀地分布在所在存储池的所有磁盘上，
这样数据在存储池的各个磁盘上是平衡分布的，从而最大化磁盘利用率，并提高磁盘的平均使用寿命。

\subsection{高可靠性}

\subsubsection{多副本}

采用多副本数据冗余机制，多个副本之间采用强一致性协议来保障数据一致性。
对N个副本的系统（N=2、3)，可以容忍N-1个副本发生故障。

多个副本按故障域策略分布在不同的故障域里，从而进一步降低了多个副本同时发生故障的概率。

对读操作而言，系统智能地按照线上工作负载选择合适的副本，以提升读性能。

\subsubsection{故障域}

故障域规则指的是：一个数据块的各副本存在不同的故障域里。
通常按节点、机架等集群拓扑结构定义故障域。

系统在任何情况下都不能违反故障域规则。

通过故障域机制，可以降低多副本同时发生故障的概率，有效地提升了系统可靠性。

\subsubsection{快速修复}

在检测到故障时，系统按存储池自动执行修复任务。

修复过程采用并行架构，有多个源盘和目标盘参与恢复工作，可以快速修复，
同时可以通过QoS策略按需控制恢复过程占用的带宽资源。

\subsection{线性扩展性}
