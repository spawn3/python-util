\section{08}

\subsection{01}

dmidecode可以查询服务器型号

\subsection{02}

理解target，各种各样的target。host-target之间的transport和protocol是区分的关键。
\hl{类比TCP协议栈}去理解各种新的网络技术。

tgtctl是target和storage的交接点，体现在文件\hl{nvmf\_suzaku\_io}里。

spdk的NVMf导出bdev。如何对接分布式存储？

把libnvme用git管理起来\todo{git-libnvme}。

尝试用一台vm把suzaku跑起来。看看具体要求和配置是什么？

完善关键流程，补上漏洞。采用\hl{用系统来工作}的理念，完善过程。

test是什么状态？应该怎么做？

hazard相关文档。

排兵布阵，上知天文下知地理。

NVMe中buffer的表示，sge？

\subsection{06}

通过ipmi控制服务器。

一块nvme盘加不上，不知为什么？51，52，53上都是如此。51重新插拔盘解决，52、53拔掉电源，重启解决。

实则性能不如8.1版，为什么？观察到disk延迟高，对disk单独进行性能测试，剔除慢盘。
用4盘测试，性能达到600w+，但latency double了。

测量每块盘的平均队列深度和延时。为什么disk的latency突然变大了呢？
\begin{myeasylist}{itemize}
& 没有读过的盘，非稳态性能？
& bactl有问题？
& remote first后，iops显著下降，latency显著升高，磁盘压力小
& \hl{把单卷大小改为80G之后，性能提升上去了}。
\end{myeasylist}

mds\_rpc\_paget，并发高，导致rangectl的内存耗尽？

加入节点，rehash，等待lease timeout，io会中断。

三个client不要同时启动，而是错开几秒钟。

rdma 在提交和完成之间，可能会占用大量内存，导致内存耗尽。怎么解决？
内存不足时使用后备内存，以处理峰值情况。或者core内存管理动态化。

\hl{拆分为两个库，都需要用静态库}，不能用so。

\subsection{07}

运行起来\hl{softroce}，更容易构建测试环境。

setenforce 0

\subsection{08}

core配比

超线程？

中断平衡，用万兆以太网卡。

login后，配置设备队列深度和调度算法。

\subsection{09}

\subsection{12}

找到了分析内存泄漏的新方法: trace方法。然后用python脚本分析，即可发现哪里分配了，而没有释放。

用两个版本来解决一致性问题：epoch、clock。epoch用来标记故障，clock用来跟踪对象修订号。
明确此两个版本的维护和规则。

看各个ctl的rpc功能，即可理解各个ctl的职责，包括：frctl, rangectl, mdctl, bactl等。
\hl{tgtctl只是解析前端请求，然后派遣给frctl去执行}。

\subsection{19}

识别不到nvme，掉电后可以重新识别，是memblaze的bug？

128k以上的io，libnvme写入不成功，内部按128k做了拆分，为什么？

hazard在本地有ib的情况下，报错，与ifconfig有关？ifconfig 2> /dev/null后也不行，为什么？
转而用vm测试。\hl{整个过程需要管理起来}。

tgtctl去task改造后性能有一定提升，如何表达sche\_sleep等错误处理逻辑呢？

dm8如何使用导出的盘？需要启用RDMA，否则性能不够好。
性能优化还能做什么？fio client irq影响大。

取法乎上，高标准是正确选择。

排查一致性问题效率太低。

单节点上一个卷可以导出4条路径，配置成多路径后，性能无法达到聚合性能，大约20w左右。
所以，linux的multipath无法产品化。

目前target以\hl{iSCSI/iSER}为主，NVMf还不够成熟，用户态的NVMf initiator？

\subsection{20}

读华为文档，对标华为，学一家就可。

AIO的盘符会变化，需要记录到磁盘元数据上。

\subsection{21}

银河麒麟不支持ib。所以需要\hl{前端iSCSI、后端RDMA}。

\subsection{22}

client scale out，server scale up。

client的cpu耗尽。中断分布均匀性与num jobs有关。

blktrace研究fio的行为。

iet的默认配置参数，有无影响性能的？如r2t。

卷，从18卷到24卷，每个client压测8个卷，在tgt ring的情况下iops 120w。
两节点时，影响较小。三节点时，其中两节点性能下降一半左右。why？
观察到mem ring scan，则52上bactl压力较大。

再次回到六个卷，52上bactl依然压力大。

\hl{单个bactl线程上的counter特别大}，普通比另外两个节点上的大很多。

采用tgt ring版才会出现影响较大的情况，难道是tgtctl不再是瓶颈后，
压力传导到bactl，数据不均衡所致？

200G的卷，切换成80G的卷试试，是否大卷跨range修改引发的问题。

开发判断数据分布平衡性的工具，如不平衡，执行再平衡操作。

\hrulefill

\hl{资源管理，产品化方面有很大工作量}。

CRUD操作

scale out：add node

scale up：add disk

删除disk、node、纳入recovery和balance任务。

pool操作

volume操作
\begin{myeasylist}{itemize}
& rm
\end{myeasylist}

\subsection{23}

bactl通过cds\_rpc提供服务。

diskid是全局资源，注册在etcd上。副本位置即按diskid指定。

0.config的0是disk slot里的索引，由diskid找到该idx，并进而定位到对应的disk。
disk->device是可变的，需要时刻保持正确无误。

diskid可以转化为对应的nid(对应bactl coremask)，chkid hash到对应的bactl coreid上。
diskid保存在diskinfo元数据区。bactl再访问对应的disk，由此可见，bactl与disk之间是多对多的关系，
不是让一个bactl负责整块disk，单块disk是并发访问的。有性能问题吗？最佳访问模式是什么？

diskid (global) -> disk slot (local) -> disk，通过mapping实现了disk slot的zero-based。
\hl{这层mapping限制了disk总数和diskid取值范围}。

为什么会观察到某个bactl负载过高的现象呢？

有独立线程周期性scan /opt/suzaku/data/ioctl/config目录下的disk配置文件，可以发现新的盘。
\hl{无diskinfo的新盘如何处理}？ scan阶段找不到对应diskinfo信息，按new disk处理？

Redis里存放了什么信息？

\subsection{24}

把suzaku作为一个整体来考虑其产品化的进度。

哪些是全闪特有的设计考虑？

恢复性能：要考虑原chunk和目标chunk的并行度。如一个磁盘故障的情况下，目标节点是否优先选择原来的节点？
并行度极大地影响性能。

单节点单盘能运行系统吗？

阵列产品中，控制器是个重量级的概念，通过控制器导出lun。
从控制器的角度来看陈列和分布式架构。suzaku的每个节点导出的\hl{tgtctl可以看作控制器}。

rangectl hash ring采用静态hash如何？

core连接数会不会成为扩展的瓶颈？

iscsi的恢复机制，阻塞io不返还，initiator如何处理？

python/c的边界，什么时候用python，什么时候用c？

命令行工具的输出，是否采用json格式？

多点写：盘阵采用共享内存，不涉及该问题，分布式系统采用该问题？

\subsection{27}

注意\hl{cmake里相关特性开关，以及suzaku.yml的配置选项}。

load disk时，如果clusterid没有变化，如果disk uuid发生变化，则load失败。
如果clusterid发生变化，则作为new disk加入。

单节点单磁盘可以启动，两个网络，solomode，副本数为1。\hl{通过同一节点login iscsi，似乎有问题}。volume\_read1 timeout。
通过另一节点则可以。

iSCSI采用ring wait的同步方式，iSER采用异步方式与frctl通信。

\hl{强化docker/simpletest test}，把单元测试也集成进去。

如何互查volid和name？

统一的任务处理框架？

采用\hl{ssan.work的mq和线程池}框架？

zk可以作为mq使用？

为什么节点内磁盘漂移，修正后，数据vdbench校验不对？

\subsection{28}

iSCSI/iSER是不是适合cpu的混合模式？每个tgtctl能力有限，用更多的tgtctl。

iSER直接返回，也只有28w iops？why？与r2t有关？

与spc-1相比，spc-1是混合读写模式。用到48个卷，iops跑到80w+，disk已充分利用。
disk大小不相等，如果相等，则可以有更好表现。

宏杉600w+，是混合读写模式。其硬件配置如何？

fio client的中断处理？

数据存放位置：
\begin{myeasylist}{itemize}
& etcd
& /opt/suzaku/data/ioctl
& /dev/shm/suzaku
\end{myeasylist}

\subsection{29}

DM8如何利用多卷？

dm8的数据文件只能放在一块盘上，怎么提升单卷性能？raid、lvm、multipath？

即使是asm，也是如此，数据文件只能放在单卷上。

实际不行用nvmf。

\hl{opensmd没有启动，导致RDMA无法连接，网络就不通}。
NVMf的检查清单
\begin{myeasylist}{itemize}
& 网络是否通：opensmd
& server短是否监听了相关port
& mlnx安装时是否开启了--with-nvmf选项
& nvme-cli 是否开启了--with-rdma选项
\end{myeasylist}

\subsection{30}

