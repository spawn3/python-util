\section{08}

\subsection{01}

dmidecode可以查询服务器型号

\subsection{02}

理解target，各种各样的target。host-target之间的transport和protocol是区分的关键。
\hl{类比TCP协议栈}去理解各种新的网络技术。

tgtctl是target和storage的交接点，体现在文件\hl{nvmf\_suzaku\_io}里。

spdk的NVMf导出bdev。如何对接分布式存储？

把libnvme用git管理起来\todo{git-libnvme}。

尝试用一台vm把suzaku跑起来。看看具体要求和配置是什么？

完善关键流程，补上漏洞。采用\hl{用系统来工作}的理念，完善过程。

test是什么状态？应该怎么做？

hazard相关文档。

排兵布阵，上知天文下知地理。

NVMe中buffer的表示，sge？

\subsection{06}

通过ipmi控制服务器。

一块nvme盘加不上，不知为什么？51，52，53上都是如此。51重新插拔盘解决，52、53拔掉电源，重启解决。

实则性能不如8.1版，为什么？观察到disk延迟高，对disk单独进行性能测试，剔除慢盘。
用4盘测试，性能达到600w+，但latency double了。

测量每块盘的平均队列深度和延时。为什么disk的latency突然变大了呢？
\begin{myeasylist}{itemize}
& 没有读过的盘，非稳态性能？
& bactl有问题？
& remote first后，iops显著下降，latency显著升高，磁盘压力小
& \hl{把单卷大小改为80G之后，性能提升上去了}。
\end{myeasylist}

mds\_rpc\_paget，并发高，导致rangectl的内存耗尽？

加入节点，rehash，等待lease timeout，io会中断。

三个client不要同时启动，而是错开几秒钟。

rdma 在提交和完成之间，可能会占用大量内存，导致内存耗尽。怎么解决？
内存不足时使用后备内存，以处理峰值情况。或者core内存管理动态化。

\hl{拆分为两个库，都需要用静态库}，不能用so。

\subsection{07}

运行起来\hl{softroce}，更容易构建测试环境。

setenforce 0

\subsection{08}

core配比

超线程？

中断平衡，用万兆以太网卡。

login后，配置设备队列深度和调度算法。

\subsection{09}

\subsection{12}

找到了分析内存泄漏的新方法: trace方法。然后用python脚本分析，即可发现哪里分配了，而没有释放。

用两个版本来解决一致性问题：epoch、clock。epoch用来标记故障，clock用来跟踪对象修订号。
明确此两个版本的维护和规则。

看各个ctl的rpc功能，即可理解各个ctl的职责，包括：frctl, rangectl, mdctl, bactl等。
\hl{tgtctl只是解析前端请求，然后派遣给frctl去执行}。

\subsection{19}

识别不到nvme，掉电后可以重新识别，是memblaze的bug？

128k以上的io，libnvme写入不成功，内部按128k做了拆分，为什么？

hazard在本地有ib的情况下，报错，与ifconfig有关？ifconfig 2> /dev/null后也不行，为什么？
转而用vm测试。\hl{整个过程需要管理起来}。

tgtctl去task改造后性能有一定提升，如何表达sche\_sleep等错误处理逻辑呢？

dm8如何使用导出的盘？需要启用RDMA，否则性能不够好。
性能优化还能做什么？fio client irq影响大。

取法乎上，高标准是正确选择。

排查一致性问题效率太低。

单节点上一个卷可以导出4条路径，配置成多路径后，性能无法达到聚合性能，大约20w左右。
所以，linux的multipath无法产品化。

目前target以\hl{iSCSI/iSER}为主，NVMf还不够成熟，用户态的NVMf initiator？

\subsection{20}

读华为文档，对标华为，学一家就可。

AIO的盘符会变化，需要记录到磁盘元数据上。

\subsection{21}

银河麒麟不支持ib。所以需要\hl{前端iSCSI、后端RDMA}。

\subsection{22}

client scale out，server scale up。

client的cpu耗尽。中断分布均匀性与num jobs有关。

blktrace研究fio的行为。

iet的默认配置参数，有无影响性能的？如r2t。

卷，从18卷到24卷，每个client压测8个卷，在tgt ring的情况下iops 120w。
两节点时，影响较小。三节点时，其中两节点性能下降一半左右。why？
观察到mem ring scan，则52上bactl压力较大。

再次回到六个卷，52上bactl依然压力大。

\hl{单个bactl线程上的counter特别大}，普通比另外两个节点上的大很多。

采用tgt ring版才会出现影响较大的情况，难道是tgtctl不再是瓶颈后，
压力传导到bactl，数据不均衡所致？

200G的卷，切换成80G的卷试试，是否大卷跨range修改引发的问题。

开发判断数据分布平衡性的工具，如不平衡，执行再平衡操作。

\hrulefill

\hl{资源管理，产品化方面有很大工作量}。

CRUD操作

scale out：add node

scale up：add disk

删除disk、node、纳入recovery和balance任务。

pool操作

volume操作
\begin{myeasylist}{itemize}
& rm
\end{myeasylist}

\subsection{23}

bactl通过cds\_rpc提供服务。

diskid是全局资源，注册在etcd上。副本位置即按diskid指定。

0.config的0是disk slot里的索引，由diskid找到该idx，并进而定位到对应的disk。
disk->device是可变的，需要时刻保持正确无误。

diskid可以转化为对应的nid(对应bactl coremask)，chkid hash到对应的bactl coreid上。
diskid保存在diskinfo元数据区。bactl再访问对应的disk，由此可见，bactl与disk之间是多对多的关系，
不是让一个bactl负责整块disk，单块disk是并发访问的。有性能问题吗？最佳访问模式是什么？

diskid (global) -> disk slot (local) -> disk，通过mapping实现了disk slot的zero-based。
\hl{这层mapping限制了disk总数和diskid取值范围}。

为什么会观察到某个bactl负载过高的现象呢？

有独立线程周期性scan /opt/suzaku/data/ioctl/config目录下的disk配置文件，可以发现新的盘。
\hl{无diskinfo的新盘如何处理}？ scan阶段找不到对应diskinfo信息，按new disk处理？

Redis里存放了什么信息？
