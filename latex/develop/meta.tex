\chapter{元数据管理}

etcd，存储池，目录，卷，快照，映射

结合lsv，row snapshot考虑当前的元数据管理，看看会有哪些瓶颈？

\section{元数据}

最重要是chkinfo结构体，表示一个chunk的若干属性，包括副本位置信息。

区分磁盘结构和内存结构。

三级元数据。是一颗大树。集群/存储池/目录/卷/chunk/replica。

\section{一致性协议}

元数据和数据的更新方式为何不同？

块存储的顺序一致性要求。成功返回的各副本强一致性，
不成功返回的，各副本可能处在不一致的状态，且无需修复。

元数据是否会进入某种不一致的状态？在整体掉电的情况下。

sqlite错误记录，指向无效的chunk？

元数据写入是sync的

事务操作，简单模型和组合模型。

\subsection{clock的作用}

分析三部曲：先分析正常情况，后分析并发场景，然后分析异常情况，
有各种各样的故障，逐一加以说明。

正常情况下，用clock维护一个chunk各副本的一致性。

每次io，携带有clock，即是chkstat.chkstat\_clock。在副本级，维护有chkid到<clock,dirty>的映射关系。
每个副本按clock递增(+1)的顺序写入。

重新载入卷，chkstat.chk\_clock设为0。

恢复一个卷的顺序，影响到并发度。

chkstat.chkstat\_clock与replica.clock相等时，说明本次IO写入完成。

写入过程，副本上的io更新序，wlist。

chunk.clock与replica.clock有双向同步的关系。一开始，chunk.clock=0，随着io.clock传播到各副本。
各副本按io.clock的递增顺序，依次完成每个io。重新加载时，通过fully过程，选择一clean副本，
并令chunk.clock=replica.clock，并以该副本为基准，对比修复余下的副本。在写入都正常完成的情况下，
chunk.clock恒等于各副本clock。

各副本写入前，let dirty=1；写入完成后，let dirty=0。通过此标记位来跟踪clock状态。

RM就地编辑，没有记录日志，所以无法应用REDO/UNDO恢复策略。
对已应答的记录，执行REDO策略；对未应答的记录，执行UNDO策略。

更新chunk.clock后，如果没有成功地传播到副本上，则该chunk上的写不可继续。

\hl{元数据采用与raw一样的一致性等级，是否合理}？ 写meta和raw不同，与此有关？

\subsection{若干故障情况}

每个chunk的各副本，状态由几部分构成：卷控制器上的chkinfo和chkstat，
replica级别的clock信息，网络ltime。

分析每一个条件：
\begin{compactenum}
\item offline
\item reset (ltime是与nid的网络连接状态，异常时重置)
\item replica status
\item chkstat.chkstat\_clock (chunk级，io级，不需持久化，每次重启设为0)
\item clock and its dirty status
\end{compactenum}

一个节点clock丢失，所有写流程都会到达这个节点（拔盘的情况与此类似）

在chunk\_push前，会从replica上同步clockstat，然后对比。

一个副本clock丢失

所有clock丢失

重启整个系统，

\section{分配}

精简配置

\section{回收}

\section{加载}

\section{恢复}

\section{均衡}
